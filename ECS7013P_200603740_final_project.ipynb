{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "adee6ef9-8529-43b1-b59e-43320951e4a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final project final code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b0c9be9-c671-42d8-9289-5640fece6f7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openunmix\n",
      "  Using cached openunmix-1.2.1-py3-none-any.whl (46 kB)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.9/site-packages (from openunmix) (4.62.3)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.9/site-packages (from openunmix) (1.20.3)\n",
      "Requirement already satisfied: torchaudio>=0.9.0 in /opt/conda/lib/python3.9/site-packages (from openunmix) (0.10.1+cu113)\n",
      "Requirement already satisfied: torch>=1.9.0 in /opt/conda/lib/python3.9/site-packages (from openunmix) (1.10.1+cu113)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.9/site-packages (from torch>=1.9.0->openunmix) (4.0.1)\n",
      "Installing collected packages: openunmix\n",
      "Successfully installed openunmix-1.2.1\n"
     ]
    }
   ],
   "source": [
    "!pip install openunmix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4cae7790-13ac-43e6-a466-4b17713fbef9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: soundfile in /opt/conda/lib/python3.9/site-packages (0.10.3.post1)\n",
      "Requirement already satisfied: cffi>=1.0 in /opt/conda/lib/python3.9/site-packages (from soundfile) (1.15.0)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.9/site-packages (from cffi>=1.0->soundfile) (2.21)\n"
     ]
    }
   ],
   "source": [
    "!pip install soundfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e58f727-a859-464d-bb99-2cebb63ce790",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Using cached tensorflow-2.8.0-cp39-cp39-manylinux2010_x86_64.whl (497.6 MB)\n",
      "Requirement already satisfied: numpy>=1.20 in /opt/conda/lib/python3.9/site-packages (from tensorflow) (1.20.3)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.9/site-packages (from tensorflow) (1.16.0)\n",
      "Collecting google-pasta>=0.1.1\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Collecting libclang>=9.0.1\n",
      "  Using cached libclang-13.0.0-py2.py3-none-manylinux1_x86_64.whl (14.5 MB)\n",
      "Collecting absl-py>=0.4.0\n",
      "  Using cached absl_py-1.0.0-py3-none-any.whl (126 kB)\n",
      "Collecting grpcio<2.0,>=1.24.3\n",
      "  Using cached grpcio-1.44.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.3 MB)\n",
      "Collecting keras-preprocessing>=1.1.1\n",
      "  Using cached Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /opt/conda/lib/python3.9/site-packages (from tensorflow) (3.6.0)\n",
      "Collecting keras<2.9,>=2.8.0rc0\n",
      "  Using cached keras-2.8.0-py2.py3-none-any.whl (1.4 MB)\n",
      "Collecting tf-estimator-nightly==2.8.0.dev2021122109\n",
      "  Using cached tf_estimator_nightly-2.8.0.dev2021122109-py2.py3-none-any.whl (462 kB)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in /opt/conda/lib/python3.9/site-packages (from tensorflow) (3.19.2)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.9/site-packages (from tensorflow) (60.5.0)\n",
      "Collecting opt-einsum>=2.3.2\n",
      "  Using cached opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "Collecting termcolor>=1.1.0\n",
      "  Using cached termcolor-1.1.0-py3-none-any.whl\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.9/site-packages (from tensorflow) (4.0.1)\n",
      "Collecting tensorboard<2.9,>=2.8\n",
      "  Using cached tensorboard-2.8.0-py3-none-any.whl (5.8 MB)\n",
      "Collecting gast>=0.2.1\n",
      "  Using cached gast-0.5.3-py3-none-any.whl (19 kB)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1\n",
      "  Using cached tensorflow_io_gcs_filesystem-0.24.0-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.1 MB)\n",
      "Collecting wrapt>=1.11.0\n",
      "  Using cached wrapt-1.14.0-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (77 kB)\n",
      "Collecting flatbuffers>=1.12\n",
      "  Using cached flatbuffers-2.0-py2.py3-none-any.whl (26 kB)\n",
      "Collecting astunparse>=1.6.0\n",
      "  Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.9/site-packages (from astunparse>=1.6.0->tensorflow) (0.37.1)\n",
      "Collecting google-auth<3,>=1.6.3\n",
      "  Downloading google_auth-2.6.4-py2.py3-none-any.whl (156 kB)\n",
      "     |████████████████████████████████| 156 kB 7.4 MB/s            \n",
      "\u001b[?25hCollecting werkzeug>=0.11.15\n",
      "  Using cached Werkzeug-2.1.1-py3-none-any.whl (224 kB)\n",
      "Collecting tensorboard-plugin-wit>=1.6.0\n",
      "  Using cached tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.9/site-packages (from tensorboard<2.9,>=2.8->tensorflow) (2.27.1)\n",
      "Collecting tensorboard-data-server<0.7.0,>=0.6.0\n",
      "  Using cached tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Using cached google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
      "Collecting markdown>=2.6.8\n",
      "  Using cached Markdown-3.3.6-py3-none-any.whl (97 kB)\n",
      "Collecting rsa<5,>=3.1.4\n",
      "  Using cached rsa-4.8-py3-none-any.whl (39 kB)\n",
      "Collecting cachetools<6.0,>=2.0.0\n",
      "  Using cached cachetools-5.0.0-py3-none-any.whl (9.1 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Using cached pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Using cached requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /opt/conda/lib/python3.9/site-packages (from markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow) (4.10.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (1.26.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (2.0.10)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (3.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (2021.10.8)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.9/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow) (3.7.0)\n",
      "Collecting pyasn1<0.5.0,>=0.4.6\n",
      "  Using cached pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.9/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow) (3.1.1)\n",
      "Installing collected packages: pyasn1, rsa, pyasn1-modules, cachetools, requests-oauthlib, google-auth, werkzeug, tensorboard-plugin-wit, tensorboard-data-server, markdown, grpcio, google-auth-oauthlib, absl-py, wrapt, tf-estimator-nightly, termcolor, tensorflow-io-gcs-filesystem, tensorboard, opt-einsum, libclang, keras-preprocessing, keras, google-pasta, gast, flatbuffers, astunparse, tensorflow\n",
      "Successfully installed absl-py-1.0.0 astunparse-1.6.3 cachetools-5.0.0 flatbuffers-2.0 gast-0.5.3 google-auth-2.6.4 google-auth-oauthlib-0.4.6 google-pasta-0.2.0 grpcio-1.44.0 keras-2.8.0 keras-preprocessing-1.1.2 libclang-13.0.0 markdown-3.3.6 opt-einsum-3.3.0 pyasn1-0.4.8 pyasn1-modules-0.2.8 requests-oauthlib-1.3.1 rsa-4.8 tensorboard-2.8.0 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tensorflow-2.8.0 tensorflow-io-gcs-filesystem-0.24.0 termcolor-1.1.0 tf-estimator-nightly-2.8.0.dev2021122109 werkzeug-2.1.1 wrapt-1.14.0\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f71a73c-c4a8-48e5-971e-6e2eafe18419",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow_io\n",
      "  Using cached tensorflow_io-0.24.0-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (23.4 MB)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem==0.24.0 in /opt/conda/lib/python3.9/site-packages (from tensorflow_io) (0.24.0)\n",
      "Installing collected packages: tensorflow-io\n",
      "Successfully installed tensorflow-io-0.24.0\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow_io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a371d708-4909-433b-8037-e6dce7a5263b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dependencies\n",
    "\n",
    "# import torch for use with Open-Unmix\n",
    "import torch\n",
    "import torchaudio\n",
    "\n",
    "# import tensorflow for CNN architecture\n",
    "import tensorflow as tf\n",
    "import tensorflow_io as tfio\n",
    "\n",
    "import numpy as np\n",
    "import scipy\n",
    "import os\n",
    "from IPython.display import Audio, display\n",
    "\n",
    "import librosa\n",
    "import sklearn\n",
    "import math\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as nnF\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "import soundfile\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "from librosa.feature import melspectrogram as mel_spec\n",
    "\n",
    "from openunmix import predict as UMXpredict\n",
    "\n",
    "# set to GPU if available\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f6883fb5-a923-4fc7-9756-ab70faba34b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN/BUILD CNN\n",
    "\n",
    "# path to IRMAS training data \n",
    "train_ds  = os.path.expanduser(\"~/teaching_material/own_work/train_data\")\n",
    "\n",
    "# path to drums training data\n",
    "drum_ds  = os.path.expanduser(\"~/drum_data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "507a568b-0f87-4f23-8305-42f4555e5816",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to shuffle two arrays in same permutation\n",
    "def shuffled_arrays(x, y):\n",
    "    p = np.random.permutation(x.shape[0])\n",
    "    return x[p], y[p]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3a0347f2-cdce-4d04-bd05-19e9c19520bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to convert training data into spectrograms\n",
    "def convert_audio_to_spectrogram(infile, win_size = 0.05, hop_size = 0.025, n_mels = 64):\n",
    "   \n",
    "    # load audio file and get sample rate\n",
    "    snd, fs = librosa.load(infile)\n",
    "    \n",
    "    # compute mel spec parameters\n",
    "    win_len = int(win_size * fs)\n",
    "    hop_len = int(hop_size * fs)\n",
    "    \n",
    "    # compute mel spectrogram from raw audio\n",
    "    spec = mel_spec(snd, sr=fs, n_mels=n_mels, hop_length=hop_len, \n",
    "                                          win_length=win_len, window='hann')\n",
    "    \n",
    "    # rescale output range to be logarithmic\n",
    "    # corresponds to human perception\n",
    "    spec = tfio.audio.dbscale(spec, top_db=80)\n",
    "    \n",
    "    # augment spectrogram in terms of frequency\n",
    "    augmented_spec = tfio.audio.freq_mask(spec, param=10)\n",
    "    \n",
    "    # convert to original spectrogram to tensor\n",
    "    # add new axis to account for channels\n",
    "    spec = spec[..., tf.newaxis]\n",
    "    spec = tf.convert_to_tensor(spec)\n",
    "    # reshape to tensor shape\n",
    "    spec = tf.reshape(spec, (64, 121, 1))\n",
    "    \n",
    "    # convert to augmented spectrogram to tensor\n",
    "    augmented_spec = augmented_spec[..., tf.newaxis]\n",
    "    augmented_spec = tf.convert_to_tensor(augmented_spec)\n",
    "    augmented_spec = tf.reshape(augmented_spec, (64, 121, 1))\n",
    "    \n",
    "    # output both\n",
    "    return spec, augmented_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d85f0949-0b48-4412-96ed-45b7b8162d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to get training data labels from file name\n",
    "def get_label(infile):\n",
    "    \n",
    "    # get section of file name associated with instrument\n",
    "    file_name = infile[1:4] \n",
    "\n",
    "    # encode as one-hot vectors\n",
    "    if file_name == 'cel':\n",
    "        label = [1,0,0,0,0,0,0,0,0,0,0,0,0]\n",
    "        \n",
    "    elif file_name == 'cla':\n",
    "        label = [0,1,0,0,0,0,0,0,0,0,0,0,0]\n",
    "        \n",
    "    elif file_name == 'flu':\n",
    "        label = [0,0,1,0,0,0,0,0,0,0,0,0,0]\n",
    "        \n",
    "    elif file_name == 'gac':\n",
    "        label = [0,0,0,1,0,0,0,0,0,0,0,0,0]\n",
    "        \n",
    "    elif file_name == 'gel':\n",
    "        label = [0,0,0,0,1,0,0,0,0,0,0,0,0]\n",
    "        \n",
    "    elif file_name == 'org':\n",
    "        label = [0,0,0,0,0,1,0,0,0,0,0,0,0]\n",
    "        \n",
    "    elif file_name == 'pia':\n",
    "        label = [0,0,0,0,0,0,1,0,0,0,0,0,0]\n",
    "        \n",
    "    elif file_name == 'sax':\n",
    "        label = [0,0,0,0,0,0,0,1,0,0,0,0,0]\n",
    "        \n",
    "    elif file_name == 'tru':\n",
    "        label = [0,0,0,0,0,0,0,0,1,0,0,0,0]\n",
    "        \n",
    "    elif file_name == 'vio':\n",
    "        label = [0,0,0,0,0,0,0,0,0,1,0,0,0]\n",
    "        \n",
    "    elif file_name == 'voi':\n",
    "        label = [0,0,0,0,0,0,0,0,0,0,1,0,0]\n",
    "        \n",
    "    else:\n",
    "        label = [0,0,0,0,0,0,0,0,0,0,0,0,1]\n",
    "    \n",
    "    return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "34dc018b-b27d-4dbc-b296-5990cd7b84cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# store labels names as strings in an array for output use\n",
    "label_names = ['Cello', 'Clarinet', 'Flute', 'Acoustic Guitar', 'Electric Guitar', \n",
    "               'Organ', 'Piano', 'Sax', 'Trumpet', 'Violin', 'Voice', 'Drums', 'Other']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "392e86fb-d59e-4dd2-b9dc-d5877f368a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to convert vectorised labels back to words\n",
    "def vector_to_label(vector):\n",
    "    \n",
    "    word = \"Undefined\"\n",
    "    \n",
    "    # check one hot vector values for label in words\n",
    "    if vector[0] == 1:\n",
    "        word = \"Cello\"\n",
    "        \n",
    "    elif vector[1] == 1:\n",
    "        word = \"Clarinet\"\n",
    "        \n",
    "    elif vector[2] == 1:\n",
    "        word = \"Flute\"\n",
    "            \n",
    "    elif vector[3] == 1:\n",
    "        word = \"Acoustic Guitar\"\n",
    "                \n",
    "    elif vector[4] == 1:\n",
    "        word = \"Electric Guitar\"\n",
    "                    \n",
    "    elif vector[5] == 1:\n",
    "        word = \"Organ\"\n",
    "                        \n",
    "    elif vector[6] == 1:\n",
    "        word = \"Piano\"\n",
    "                            \n",
    "    elif vector[7] == 1:\n",
    "        word = \"Sax\"\n",
    "                                \n",
    "    elif vector[8] == 1:\n",
    "        word = \"Trumpet\"\n",
    "                                    \n",
    "    elif vector[9] == 1:\n",
    "        word = \"Violin\"\n",
    "                                        \n",
    "    elif vector[10] == 1:\n",
    "        word = \"Voice\"\n",
    "        \n",
    "    elif vector[11] == 1:\n",
    "        word = \"Drums\"\n",
    "\n",
    "    return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8e7b2f09-9150-4375-844b-06f2d8a04f7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3261\n"
     ]
    }
   ],
   "source": [
    "# check total number of training samples\n",
    "no_of_entries = len([name for name in os.listdir(train_ds)])\n",
    "print(no_of_entries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1bb168dc-393c-479d-a3ac-0652602fda1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-13 11:15:51.693037: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-04-13 11:16:09.437626: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3728 MB memory:  -> device: 0, name: NVIDIA A40, pci bus id: 0000:17:00.0, compute capability: 8.6\n",
      "2022-04-13 11:16:09.439361: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 41734 MB memory:  -> device: 1, name: NVIDIA A40, pci bus id: 0000:65:00.0, compute capability: 8.6\n",
      "2022-04-13 11:16:09.441080: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 41734 MB memory:  -> device: 2, name: NVIDIA A40, pci bus id: 0000:ca:00.0, compute capability: 8.6\n",
      "2022-04-13 11:16:09.442813: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:3 with 41734 MB memory:  -> device: 3, name: NVIDIA A40, pci bus id: 0000:e3:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n",
      "100\n",
      "150\n",
      "200\n",
      "250\n",
      "300\n",
      "350\n",
      "400\n",
      "450\n",
      "500\n",
      "550\n",
      "600\n",
      "650\n",
      "700\n",
      "750\n",
      "800\n",
      "850\n",
      "900\n",
      "950\n",
      "1000\n",
      "1050\n",
      "1100\n",
      "1150\n",
      "1200\n",
      "1250\n",
      "1300\n",
      "1350\n",
      "1400\n",
      "1450\n",
      "1500\n",
      "1550\n",
      "1600\n",
      "1650\n",
      "1700\n",
      "1750\n",
      "1800\n",
      "1850\n",
      "1900\n",
      "1950\n",
      "2000\n",
      "2050\n",
      "2100\n",
      "2150\n",
      "2200\n",
      "2250\n",
      "2300\n",
      "2350\n",
      "2400\n",
      "2450\n",
      "2500\n",
      "2550\n",
      "2600\n",
      "2650\n",
      "2700\n",
      "2750\n"
     ]
    }
   ],
   "source": [
    "# initialise an array for storing output spectrogram tensors\n",
    "spectrograms = np.zeros((no_of_entries * 2, 64, 121, 1))\n",
    "\n",
    "# initialise counter for counting number of spectrograms calculated\n",
    "# also used for location for storing spectrograms\n",
    "counter = 0\n",
    "\n",
    "# scan through training data directory and compute spectrograms\n",
    "for entry in os.scandir(train_ds):\n",
    "    \n",
    "    # if file is not .wav then skip\n",
    "    if str(os.listdir(train_ds)[counter][-3:]) != 'wav':\n",
    "        continue\n",
    "    \n",
    "    # add output spectrograms to spectrogram array\n",
    "    spec, augmented_spec = convert_audio_to_spectrogram(entry)\n",
    "    spectrograms[counter] = spec\n",
    "    spectrograms[counter + 1] = augmented_spec\n",
    "    # advance counter\n",
    "    counter += 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bfe887c7-0826-4453-bdec-e2fa6bfcefaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initalise array for storing label data\n",
    "labels = np.zeros((no_of_entries * 2, 13))\n",
    "\n",
    "# counter for inputting labels in array\n",
    "label_counter = 0\n",
    "\n",
    "for name in os.listdir(train_ds):\n",
    "    \n",
    "    # skip file if not .wav\n",
    "    if str(os.listdir(train_ds)[label_counter][-3:]) != 'wav':\n",
    "        continue\n",
    "    \n",
    "    # output labels to array\n",
    "    labels[label_counter] = get_label(name)\n",
    "    labels[label_counter + 1] = get_label(name)    \n",
    "    # advance counter\n",
    "    label_counter += 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8037585e-7aa4-4048-bb1d-699c3895cc7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get spectrograms and label arrays to correct length\n",
    "spectrograms = spectrograms[:counter]\n",
    "labels = labels[:label_counter]\n",
    "\n",
    "# get total number of samples from spectrogram counter\n",
    "total_length = counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6f50a3e4-96fc-48a7-bb82-0234d530ab34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/librosa/core/spectrum.py:222: UserWarning: n_fft=2048 is too small for input signal of length=184\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "520\n"
     ]
    }
   ],
   "source": [
    "# initialise array for storing drum spectrograms\n",
    "drum_spectrograms = []\n",
    "\n",
    "# loop through each file in the directory\n",
    "for drum_file in os.scandir(drum_ds):\n",
    "    \n",
    "    # initialise counter to zero\n",
    "    spec_counter = 0\n",
    "    \n",
    "    # load audio\n",
    "    snd, sr = librosa.load(drum_file)\n",
    "    \n",
    "    # calculate a three second window \n",
    "    # 3 seconds should match IRMAS data\n",
    "    three_seconds = 3 * sr\n",
    "    \n",
    "    # set spectrogram variables\n",
    "    n_mels = 64\n",
    "    win_len = int(0.05 * sr)\n",
    "    hop_len = int(0.025 * sr)\n",
    "    \n",
    "    # loop through length of audio\n",
    "    for i in range(len(snd)):\n",
    "        \n",
    "        # only calulate enough spectrograms to keep dataset balanced\n",
    "        if spec_counter > 13:\n",
    "            break\n",
    "        \n",
    "        # take splits of audio at three seconds\n",
    "        if (i % three_seconds) == 0:\n",
    "\n",
    "            # compute mel spectrogram from raw audio\n",
    "            spec = mel_spec(snd[i:i+three_seconds], sr=sr, n_mels=n_mels, hop_length=hop_len, \n",
    "                                                  win_length=win_len, window='hann')\n",
    "            \n",
    "            # advance spectrogram counter\n",
    "            spec_counter += 1\n",
    "\n",
    "            # if spectrogram is too small, skip over\n",
    "            if(spec.shape[1] < 121):\n",
    "                continue\n",
    "            \n",
    "            # rescale output range to be logarithmic\n",
    "            # corresponds to human perception\n",
    "            spec = tfio.audio.dbscale(spec, top_db=80)\n",
    "\n",
    "            # augment spectrogram in terms of frequency\n",
    "            augmented_spec = tfio.audio.freq_mask(spec, param=10)\n",
    "\n",
    "            # convert to original spectrogram to tensor\n",
    "            # add new axis to account for channels\n",
    "            spec = spec[..., tf.newaxis]\n",
    "            spec = tf.convert_to_tensor(spec)\n",
    "            # reshape to tensor shape\n",
    "            spec = tf.reshape(spec, (64, 121, 1))\n",
    "            \n",
    "            # convert to augmented spectrogram to tensor\n",
    "            augmented_spec = augmented_spec[..., tf.newaxis]\n",
    "            augmented_spec = tf.convert_to_tensor(augmented_spec)\n",
    "            augmented_spec = tf.reshape(augmented_spec, (64, 121, 1))\n",
    "           \n",
    "            # output spectrograms to array\n",
    "            drum_spectrograms.append(spec)\n",
    "            drum_spectrograms.append(augmented_spec)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a7e2aacc-5ec7-4e0e-8ec5-ef140d6fc5b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n"
     ]
    }
   ],
   "source": [
    "# initialise array for storing drum labels\n",
    "drum_labels = np.zeros((len(drum_spectrograms), 13))\n",
    "\n",
    "# attache one hot vector for evbery drum spectrogram\n",
    "for i in range(len(drum_spectrograms)):\n",
    "    drum_labels[i] = [0,0,0,0,0,0,0,0,0,0,0,1,0]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2f988261-03fb-430c-9052-b99db7398128",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle drum spectrograms \n",
    "drum_spectrograms = np.array(drum_spectrograms)\n",
    "shuffled_drum_spectrograms, shuffled_drum_labels = shuffled_arrays(drum_spectrograms, drum_labels)\n",
    "\n",
    "# keep first 250 spectrograms to keep dataset balanced\n",
    "shuffled_drum_spectrograms = shuffled_drum_spectrograms[:250]\n",
    "drum_labels = drum_labels[:250]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "802dd0e2-ab3a-4080-90b8-732924362e61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66.93445587158203\n",
      "67.121185\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_93/1840072967.py:20: RuntimeWarning: invalid value encountered in floor_divide\n",
      "  normalised_drum_specs = np.abs(shuffled_drum_spectrograms // most_max_drums)\n"
     ]
    }
   ],
   "source": [
    "# get largest value from across spectrograms\n",
    "most_max_regular = np.amax(spectrograms)\n",
    "\n",
    "# normalise all specs according to maximum value\n",
    "normalised_specs = np.abs(spectrograms // most_max_regular)\n",
    "# if any spectrograms NAN, replace with 0\n",
    "normalised_specs = np.where(np.isnan(normalised_specs), 0, normalised_specs)\n",
    "\n",
    "# same for drums - get largest value\n",
    "most_max_drums = np.amax(shuffled_drum_spectrograms)\n",
    "\n",
    "# normalise all specs according to maximum value\n",
    "normalised_drum_specs = np.abs(shuffled_drum_spectrograms // most_max_drums)\n",
    "# if any spectrograms NAN, replace with 0\n",
    "normalised_drum_specs = np.where(np.isnan(normalised_drum_specs), 0, normalised_drum_specs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "00eae109-5787-4a9e-99b7-4ad746417829",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3044, 64, 121, 1)\n",
      "(3044, 13)\n"
     ]
    }
   ],
   "source": [
    "# concatenate IRMAS and drum data sepctrograms into one array\n",
    "all_spectrograms = [normalised_specs, normalised_drum_specs]\n",
    "all_spectrograms = np.concatenate(all_spectrograms)\n",
    "\n",
    "# concatenate IRMAS and drum data labels into one array\n",
    "all_labels = [labels, drum_labels]\n",
    "all_labels = np.concatenate(all_labels)\n",
    "\n",
    "# print shapes for checking/debugging\n",
    "print(all_spectrograms.shape)\n",
    "print(all_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "27017baa-8f4e-4add-9057-f7c83510c619",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle spectrograms and labels with same permutation \n",
    "suffled_spectrograms, shuffled_labels = shuffled_arrays(all_spectrograms, all_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c4a98499-5e0b-4f4e-9333-df0ee93f9eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into train and test\n",
    "# make train data 80% of all data\n",
    "train_specs = suffled_spectrograms[:int(total_length * 0.8)]\n",
    "\n",
    "# val data is 10%\n",
    "val_specs = suffled_spectrograms[int(total_length * 0.8):int(total_length * 0.9)]\n",
    "\n",
    "# test data is the last 10%\n",
    "test_specs = suffled_spectrograms[int(total_length * 0.9):total_length]\n",
    "\n",
    "# split labels same way as spectrograms\n",
    "train_labels = shuffled_labels[:int(total_length * 0.8)]\n",
    "val_labels = shuffled_labels[int(total_length * 0.8):int(total_length * 0.9)]\n",
    "test_labels = shuffled_labels[int(total_length * 0.9):total_length]\n",
    "\n",
    "# convert spectrograms into tensor for training\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(train_specs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d8c81833-8e52-450b-b22d-45fec194ac1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_4 (Conv2D)           (None, 63, 120, 32)       160       \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPooling  (None, 31, 60, 32)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 30, 59, 64)        8256      \n",
      "                                                                 \n",
      " max_pooling2d_4 (MaxPooling  (None, 15, 29, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_6 (Conv2D)           (None, 14, 28, 128)       32896     \n",
      "                                                                 \n",
      " max_pooling2d_5 (MaxPooling  (None, 7, 14, 128)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 7, 14, 128)       512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_7 (Conv2D)           (None, 6, 13, 128)        65664     \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 9984)              0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1024)              10224640  \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 512)               524800    \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 13)                6669      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 10,863,597\n",
      "Trainable params: 10,863,341\n",
      "Non-trainable params: 256\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# BUILD CNN MODEL\n",
    "\n",
    "# use He initialisation\n",
    "initializer = tf.keras.initializers.HeNormal()\n",
    "\n",
    "# set up model as sequential keras model\n",
    "model = models.Sequential()\n",
    "\n",
    "# add convolutional layer followed by max pooling\n",
    "model.add(layers.Conv2D(32, (2, 2), activation='tanh', input_shape=(64, 121, 1), kernel_initializer=initializer))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "# add convolutional layer followed by max pooling\n",
    "model.add(layers.Conv2D(64, (2, 2), activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "# add convolutional layer followed by max pooling\n",
    "model.add(layers.Conv2D(128, (2, 2), activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "# normalise the batch \n",
    "model.add(layers.BatchNormalization(momentum = 0.9))\n",
    "\n",
    "# add convolutional layer - this is ususally 16 not 128\n",
    "model.add(layers.Conv2D(128, (2, 2), activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)))\n",
    "\n",
    "# flatten\n",
    "model.add(layers.Flatten())\n",
    "\n",
    "# three dense layers in sequence to 12 possibel outputs\n",
    "model.add(layers.Dense(1024, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)))\n",
    "model.add(layers.Dense(512, activation='tanh', kernel_regularizer=tf.keras.regularizers.l2(0.001)))\n",
    "# final dense layer with 13 outputs - each output corresponds to possible label\n",
    "model.add(layers.Dense(13, activation='softmax'))\n",
    "\n",
    "# summarise models\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e4e8a0be-ec20-4aa9-8e4e-a6604f9b8f82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: LearningRateScheduler setting learning rate to 1e-05.\n",
      "Epoch 1/50\n",
      "70/70 [==============================] - 2s 22ms/step - loss: 5.0500 - accuracy: 0.2367 - val_loss: 4.9078 - val_accuracy: 0.2545 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 2: LearningRateScheduler setting learning rate to 1e-05.\n",
      "Epoch 2/50\n",
      "70/70 [==============================] - 1s 17ms/step - loss: 4.6538 - accuracy: 0.4054 - val_loss: 4.7507 - val_accuracy: 0.3262 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 3: LearningRateScheduler setting learning rate to 1e-05.\n",
      "Epoch 3/50\n",
      "70/70 [==============================] - 1s 18ms/step - loss: 4.4116 - accuracy: 0.5159 - val_loss: 4.6521 - val_accuracy: 0.3692 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 4: LearningRateScheduler setting learning rate to 1e-05.\n",
      "Epoch 4/50\n",
      "70/70 [==============================] - 1s 18ms/step - loss: 4.2157 - accuracy: 0.5928 - val_loss: 4.5334 - val_accuracy: 0.4194 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 5: LearningRateScheduler setting learning rate to 1e-05.\n",
      "Epoch 5/50\n",
      "70/70 [==============================] - 1s 18ms/step - loss: 4.0369 - accuracy: 0.6720 - val_loss: 4.4686 - val_accuracy: 0.4337 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 6: LearningRateScheduler setting learning rate to 1e-05.\n",
      "Epoch 6/50\n",
      "70/70 [==============================] - 1s 18ms/step - loss: 3.8876 - accuracy: 0.7342 - val_loss: 4.3476 - val_accuracy: 0.5090 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 7: LearningRateScheduler setting learning rate to 1e-05.\n",
      "Epoch 7/50\n",
      "70/70 [==============================] - 1s 18ms/step - loss: 3.7459 - accuracy: 0.7852 - val_loss: 4.2551 - val_accuracy: 0.5627 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 8: LearningRateScheduler setting learning rate to 1e-05.\n",
      "Epoch 8/50\n",
      "70/70 [==============================] - 1s 17ms/step - loss: 3.6190 - accuracy: 0.8273 - val_loss: 4.2046 - val_accuracy: 0.5556 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 9: LearningRateScheduler setting learning rate to 1e-05.\n",
      "Epoch 9/50\n",
      "70/70 [==============================] - 1s 18ms/step - loss: 3.5069 - accuracy: 0.8613 - val_loss: 4.1155 - val_accuracy: 0.5914 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 10: LearningRateScheduler setting learning rate to 1e-05.\n",
      "Epoch 10/50\n",
      "70/70 [==============================] - 1s 18ms/step - loss: 3.4048 - accuracy: 0.8868 - val_loss: 4.0580 - val_accuracy: 0.6129 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 11: LearningRateScheduler setting learning rate to 1e-05.\n",
      "Epoch 11/50\n",
      "70/70 [==============================] - 1s 18ms/step - loss: 3.3082 - accuracy: 0.9172 - val_loss: 3.9660 - val_accuracy: 0.6487 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 12: LearningRateScheduler setting learning rate to 1e-05.\n",
      "Epoch 12/50\n",
      "70/70 [==============================] - 1s 19ms/step - loss: 3.2241 - accuracy: 0.9315 - val_loss: 3.8895 - val_accuracy: 0.6882 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 13: LearningRateScheduler setting learning rate to 1e-05.\n",
      "Epoch 13/50\n",
      "70/70 [==============================] - 1s 19ms/step - loss: 3.1499 - accuracy: 0.9499 - val_loss: 3.8508 - val_accuracy: 0.7061 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 14: LearningRateScheduler setting learning rate to 1e-05.\n",
      "Epoch 14/50\n",
      "70/70 [==============================] - 1s 18ms/step - loss: 3.0843 - accuracy: 0.9544 - val_loss: 3.7919 - val_accuracy: 0.7240 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 15: LearningRateScheduler setting learning rate to 1e-05.\n",
      "Epoch 15/50\n",
      "70/70 [==============================] - 1s 18ms/step - loss: 3.0192 - accuracy: 0.9669 - val_loss: 3.7367 - val_accuracy: 0.7348 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 16: LearningRateScheduler setting learning rate to 1e-05.\n",
      "Epoch 16/50\n",
      "70/70 [==============================] - 1s 18ms/step - loss: 2.9659 - accuracy: 0.9732 - val_loss: 3.6868 - val_accuracy: 0.7599 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 17: LearningRateScheduler setting learning rate to 1e-05.\n",
      "Epoch 17/50\n",
      "70/70 [==============================] - 1s 18ms/step - loss: 2.9155 - accuracy: 0.9799 - val_loss: 3.6257 - val_accuracy: 0.7634 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 18: LearningRateScheduler setting learning rate to 1e-05.\n",
      "Epoch 18/50\n",
      "70/70 [==============================] - 1s 18ms/step - loss: 2.8722 - accuracy: 0.9852 - val_loss: 3.5725 - val_accuracy: 0.7670 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 19: LearningRateScheduler setting learning rate to 1e-05.\n",
      "Epoch 19/50\n",
      "70/70 [==============================] - 1s 19ms/step - loss: 2.8309 - accuracy: 0.9879 - val_loss: 3.5623 - val_accuracy: 0.7814 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 20: LearningRateScheduler setting learning rate to 1e-05.\n",
      "Epoch 20/50\n",
      "70/70 [==============================] - 1s 18ms/step - loss: 2.7937 - accuracy: 0.9915 - val_loss: 3.5041 - val_accuracy: 0.7742 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 21: LearningRateScheduler setting learning rate to 1e-05.\n",
      "Epoch 21/50\n",
      "70/70 [==============================] - 1s 19ms/step - loss: 2.7603 - accuracy: 0.9928 - val_loss: 3.4653 - val_accuracy: 0.7849 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 22: LearningRateScheduler setting learning rate to 1e-05.\n",
      "Epoch 22/50\n",
      "70/70 [==============================] - 1s 18ms/step - loss: 2.7279 - accuracy: 0.9960 - val_loss: 3.4101 - val_accuracy: 0.7742 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 23: LearningRateScheduler setting learning rate to 1e-05.\n",
      "Epoch 23/50\n",
      "70/70 [==============================] - 1s 18ms/step - loss: 2.6990 - accuracy: 0.9951 - val_loss: 3.3964 - val_accuracy: 0.7921 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 24: LearningRateScheduler setting learning rate to 1e-05.\n",
      "Epoch 24/50\n",
      "70/70 [==============================] - 1s 18ms/step - loss: 2.6719 - accuracy: 0.9982 - val_loss: 3.3655 - val_accuracy: 0.7849 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 25: LearningRateScheduler setting learning rate to 1e-05.\n",
      "Epoch 25/50\n",
      "70/70 [==============================] - 1s 18ms/step - loss: 2.6465 - accuracy: 0.9982 - val_loss: 3.3375 - val_accuracy: 0.8100 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 26: LearningRateScheduler setting learning rate to 1e-05.\n",
      "Epoch 26/50\n",
      "70/70 [==============================] - 1s 18ms/step - loss: 2.6220 - accuracy: 0.9987 - val_loss: 3.3108 - val_accuracy: 0.7957 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 27: LearningRateScheduler setting learning rate to 1e-05.\n",
      "Epoch 27/50\n",
      "70/70 [==============================] - 1s 18ms/step - loss: 2.5983 - accuracy: 0.9991 - val_loss: 3.3285 - val_accuracy: 0.7921 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 28: LearningRateScheduler setting learning rate to 1e-05.\n",
      "Epoch 28/50\n",
      "70/70 [==============================] - 1s 19ms/step - loss: 2.5752 - accuracy: 1.0000 - val_loss: 3.2624 - val_accuracy: 0.8065 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 29: LearningRateScheduler setting learning rate to 1e-05.\n",
      "Epoch 29/50\n",
      "70/70 [==============================] - 1s 19ms/step - loss: 2.5523 - accuracy: 0.9991 - val_loss: 3.2501 - val_accuracy: 0.8029 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 30: LearningRateScheduler setting learning rate to 1e-05.\n",
      "Epoch 30/50\n",
      "70/70 [==============================] - 1s 19ms/step - loss: 2.5301 - accuracy: 1.0000 - val_loss: 3.2106 - val_accuracy: 0.8029 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 31: LearningRateScheduler setting learning rate to 1e-05.\n",
      "Epoch 31/50\n",
      "70/70 [==============================] - 1s 18ms/step - loss: 2.5082 - accuracy: 0.9996 - val_loss: 3.1979 - val_accuracy: 0.8065 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 32: LearningRateScheduler setting learning rate to 1e-05.\n",
      "Epoch 32/50\n",
      "70/70 [==============================] - 1s 17ms/step - loss: 2.4865 - accuracy: 0.9996 - val_loss: 3.1824 - val_accuracy: 0.8029 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 33: LearningRateScheduler setting learning rate to 1e-05.\n",
      "Epoch 33/50\n",
      "70/70 [==============================] - 1s 17ms/step - loss: 2.4649 - accuracy: 1.0000 - val_loss: 3.1517 - val_accuracy: 0.8065 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 34: LearningRateScheduler setting learning rate to 1e-05.\n",
      "Epoch 34/50\n",
      "70/70 [==============================] - 1s 18ms/step - loss: 2.4435 - accuracy: 1.0000 - val_loss: 3.1457 - val_accuracy: 0.8136 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 35: LearningRateScheduler setting learning rate to 1e-05.\n",
      "Epoch 35/50\n",
      "70/70 [==============================] - 1s 17ms/step - loss: 2.4220 - accuracy: 1.0000 - val_loss: 3.1169 - val_accuracy: 0.8136 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 36: LearningRateScheduler setting learning rate to 1e-05.\n",
      "Epoch 36/50\n",
      "70/70 [==============================] - 1s 16ms/step - loss: 2.4013 - accuracy: 1.0000 - val_loss: 3.0981 - val_accuracy: 0.8100 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 37: LearningRateScheduler setting learning rate to 1e-05.\n",
      "Epoch 37/50\n",
      "70/70 [==============================] - 1s 16ms/step - loss: 2.3797 - accuracy: 1.0000 - val_loss: 3.0772 - val_accuracy: 0.8136 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 38: LearningRateScheduler setting learning rate to 1e-05.\n",
      "Epoch 38/50\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 2.3590 - accuracy: 1.0000 - val_loss: 3.0590 - val_accuracy: 0.8100 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 39: LearningRateScheduler setting learning rate to 1e-05.\n",
      "Epoch 39/50\n",
      "70/70 [==============================] - 3s 47ms/step - loss: 2.3378 - accuracy: 1.0000 - val_loss: 3.0351 - val_accuracy: 0.8136 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 40: LearningRateScheduler setting learning rate to 1e-05.\n",
      "Epoch 40/50\n",
      "70/70 [==============================] - 1s 17ms/step - loss: 2.3172 - accuracy: 1.0000 - val_loss: 3.0112 - val_accuracy: 0.8136 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 41: LearningRateScheduler setting learning rate to 1e-05.\n",
      "Epoch 41/50\n",
      "70/70 [==============================] - 1s 18ms/step - loss: 2.2963 - accuracy: 1.0000 - val_loss: 3.0001 - val_accuracy: 0.8065 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 42: LearningRateScheduler setting learning rate to 1e-05.\n",
      "Epoch 42/50\n",
      "70/70 [==============================] - 1s 18ms/step - loss: 2.2762 - accuracy: 1.0000 - val_loss: 2.9779 - val_accuracy: 0.8208 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 43: LearningRateScheduler setting learning rate to 1e-05.\n",
      "Epoch 43/50\n",
      "70/70 [==============================] - 1s 18ms/step - loss: 2.2557 - accuracy: 1.0000 - val_loss: 2.9591 - val_accuracy: 0.8172 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 44: LearningRateScheduler setting learning rate to 1e-05.\n",
      "Epoch 44/50\n",
      "70/70 [==============================] - 1s 17ms/step - loss: 2.2355 - accuracy: 1.0000 - val_loss: 2.9349 - val_accuracy: 0.8136 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 45: LearningRateScheduler setting learning rate to 1e-05.\n",
      "Epoch 45/50\n",
      "70/70 [==============================] - 1s 18ms/step - loss: 2.2154 - accuracy: 1.0000 - val_loss: 2.9178 - val_accuracy: 0.8208 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 46: LearningRateScheduler setting learning rate to 1e-05.\n",
      "Epoch 46/50\n",
      "70/70 [==============================] - 1s 17ms/step - loss: 2.1957 - accuracy: 1.0000 - val_loss: 2.9100 - val_accuracy: 0.8136 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 47: LearningRateScheduler setting learning rate to 1e-05.\n",
      "Epoch 47/50\n",
      "70/70 [==============================] - 3s 40ms/step - loss: 2.1758 - accuracy: 1.0000 - val_loss: 2.8869 - val_accuracy: 0.8244 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 48: LearningRateScheduler setting learning rate to 1e-05.\n",
      "Epoch 48/50\n",
      "70/70 [==============================] - 1s 18ms/step - loss: 2.1562 - accuracy: 1.0000 - val_loss: 2.8622 - val_accuracy: 0.8208 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 49: LearningRateScheduler setting learning rate to 1e-05.\n",
      "Epoch 49/50\n",
      "70/70 [==============================] - 1s 18ms/step - loss: 2.1366 - accuracy: 1.0000 - val_loss: 2.8518 - val_accuracy: 0.8172 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 50: LearningRateScheduler setting learning rate to 5e-06.\n",
      "Epoch 50/50\n",
      "70/70 [==============================] - 1s 18ms/step - loss: 2.1218 - accuracy: 1.0000 - val_loss: 2.8368 - val_accuracy: 0.8172 - lr: 5.0000e-06\n"
     ]
    }
   ],
   "source": [
    "# TRAIN THE NETWORK\n",
    "\n",
    "# initialise optimizer\n",
    "optimizer = tf.keras.optimizers.RMSprop(learning_rate=0.0001, momentum = 0.1)\n",
    "\n",
    "# compile the model\n",
    "model.compile(optimizer=optimizer,\n",
    "              loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "              metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# save model checkpoints\n",
    "callbacks = [\n",
    "             tf.keras.callbacks.ModelCheckpoint(\n",
    "                  filepath='./training_checkpoints/ckpt_{epoch}',\n",
    "                  save_weights_only=True\n",
    "             ),\n",
    "]\n",
    "\n",
    "# train the model iver 50 epochs\n",
    "history = model.fit(train_specs, train_labels, epochs=50, callbacks=callbacks,\n",
    "                   validation_data=(val_specs, val_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "90b0a369-76f8-49a1-8431-a2774b10e0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EVAULATION PORTION OF THE MODEL\n",
    "\n",
    "# get predictions of test spectrograms \n",
    "predictions = model.predict(test_specs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7a68fbba-85fc-419c-8197-825c2a9a1a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to calculate accuracies of the model\n",
    "# outputs true positive, false positives, true negatives and false negatives\n",
    "# compares ground truths to predictions for specified instrument\n",
    "def counters(ground_truths, predictions, instrument):\n",
    "    \n",
    "    # initialise counters for each outcome\n",
    "    TP = 0\n",
    "    TN = 0\n",
    "    FP = 0\n",
    "    FN = 0\n",
    "    \n",
    "    # cycle through output predictions\n",
    "    for i in range(len(predictions)):\n",
    "        \n",
    "        # check if prediction matches specified instrument\n",
    "        if predictions[i] == instrument:\n",
    "            \n",
    "            # if ground truth matches instrument, add one to true positives\n",
    "            if ground_truths[i] == instrument:\n",
    "                TP += 1\n",
    "                \n",
    "            # else if ground truth does not match instrument, add one to false positives\n",
    "            if ground_truths[i] != instrument:\n",
    "                FP += 1\n",
    "            \n",
    "        # check if prediction does not match specified instrument\n",
    "        if predictions[i] != instrument:\n",
    "            \n",
    "            # if ground truth also does not match instrument, add one to true negatives\n",
    "            if ground_truths != instrument:\n",
    "                TN += 1\n",
    "            \n",
    "            # else if ground truth is instrument, add one to false negatives\n",
    "            if ground_truths[i] == instrument:\n",
    "                FN += 1\n",
    "            \n",
    "    return TP, FP, TN, FN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c488378b-7fc8-42da-8f26-219cd64fafdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False Positive for:  Cello\n",
      "Sax\n",
      "False Positive for:  Cello\n",
      "Violin\n",
      "False Positive for:  Cello\n",
      "Acoustic Guitar\n",
      "False Positive for:  Cello\n",
      "Acoustic Guitar\n",
      "False Positive for:  Cello\n",
      "Sax\n",
      "False Positive for:  Cello\n",
      "Sax\n",
      "Cello TP: 20\n",
      "Cello FP: 6\n",
      "Cello TN: 254\n",
      "Cello FN: 7\n",
      " \n",
      "False Positive for:  Clarinet\n",
      "Piano\n",
      "False Positive for:  Clarinet\n",
      "Acoustic Guitar\n",
      "False Positive for:  Clarinet\n",
      "Electric Guitar\n",
      "False Positive for:  Clarinet\n",
      "Flute\n",
      "False Positive for:  Clarinet\n",
      "Violin\n",
      "False Positive for:  Clarinet\n",
      "Violin\n",
      "False Positive for:  Clarinet\n",
      "Trumpet\n",
      "False Positive for:  Clarinet\n",
      "Voice\n",
      "False Positive for:  Clarinet\n",
      "Sax\n",
      "False Positive for:  Clarinet\n",
      "Piano\n",
      "False Positive for:  Clarinet\n",
      "Voice\n",
      "Clarinet TP: 20\n",
      "Clarinet FP: 11\n",
      "Clarinet TN: 249\n",
      "Clarinet FN: 4\n",
      " \n",
      "False Positive for:  Flute\n",
      "Violin\n",
      "False Positive for:  Flute\n",
      "Sax\n",
      "False Positive for:  Flute\n",
      "Cello\n",
      "Flute TP: 20\n",
      "Flute FP: 3\n",
      "Flute TN: 257\n",
      "Flute FN: 4\n",
      " \n",
      "False Positive for:  Acoustic Guitar\n",
      "Sax\n",
      "False Positive for:  Acoustic Guitar\n",
      "Cello\n",
      "False Positive for:  Acoustic Guitar\n",
      "Electric Guitar\n",
      "False Positive for:  Acoustic Guitar\n",
      "Piano\n",
      "False Positive for:  Acoustic Guitar\n",
      "Trumpet\n",
      "False Positive for:  Acoustic Guitar\n",
      "Trumpet\n",
      "False Positive for:  Acoustic Guitar\n",
      "Cello\n",
      "Acoustic Guitar TP: 6\n",
      "Acoustic Guitar FP: 7\n",
      "Acoustic Guitar TN: 267\n",
      "Acoustic Guitar FN: 5\n",
      " \n",
      "False Positive for:  Electric Guitar\n",
      "Cello\n",
      "Electric Guitar TP: 22\n",
      "Electric Guitar FP: 1\n",
      "Electric Guitar TN: 257\n",
      "Electric Guitar FN: 2\n",
      " \n",
      "False Positive for:  Organ\n",
      "Sax\n",
      "False Positive for:  Organ\n",
      "Piano\n",
      "False Positive for:  Organ\n",
      "Voice\n",
      "False Positive for:  Organ\n",
      "Flute\n",
      "Organ TP: 20\n",
      "Organ FP: 4\n",
      "Organ TN: 256\n",
      "Organ FN: 3\n",
      " \n",
      "False Positive for:  Piano\n",
      "Sax\n",
      "False Positive for:  Piano\n",
      "Clarinet\n",
      "Piano TP: 13\n",
      "Piano FP: 2\n",
      "Piano TN: 265\n",
      "Piano FN: 6\n",
      " \n",
      "False Positive for:  Sax\n",
      "Clarinet\n",
      "False Positive for:  Sax\n",
      "Clarinet\n",
      "False Positive for:  Sax\n",
      "Flute\n",
      "False Positive for:  Sax\n",
      "Organ\n",
      "False Positive for:  Sax\n",
      "Acoustic Guitar\n",
      "False Positive for:  Sax\n",
      "Acoustic Guitar\n",
      "False Positive for:  Sax\n",
      "Piano\n",
      "False Positive for:  Sax\n",
      "Clarinet\n",
      "Sax TP: 21\n",
      "Sax FP: 8\n",
      "Sax TN: 251\n",
      "Sax FN: 10\n",
      " \n",
      "False Positive for:  Trumpet\n",
      "Sax\n",
      "False Positive for:  Trumpet\n",
      "Organ\n",
      "False Positive for:  Trumpet\n",
      "Flute\n",
      "False Positive for:  Trumpet\n",
      "Organ\n",
      "Trumpet TP: 25\n",
      "Trumpet FP: 4\n",
      "Trumpet TN: 251\n",
      "Trumpet FN: 3\n",
      " \n",
      "False Positive for:  Violin\n",
      "Piano\n",
      "False Positive for:  Violin\n",
      "Cello\n",
      "False Positive for:  Violin\n",
      "Cello\n",
      "False Positive for:  Violin\n",
      "Cello\n",
      "Violin TP: 19\n",
      "Violin FP: 4\n",
      "Violin TN: 257\n",
      "Violin FN: 5\n",
      " \n",
      "False Positive for:  Voice\n",
      "Sax\n",
      "Voice TP: 15\n",
      "Voice FP: 1\n",
      "Voice TN: 264\n",
      "Voice FN: 3\n",
      " \n",
      "False Positive for:  Drums\n",
      "Violin\n",
      "Drums TP: 27\n",
      "Drums FP: 1\n",
      "Drums TN: 252\n",
      "Drums FN: 0\n",
      " \n",
      "Other TP: 0\n",
      "Other FP: 0\n",
      "Other TN: 280\n",
      "Other FN: 0\n",
      " \n"
     ]
    }
   ],
   "source": [
    "# get number of predictions made\n",
    "no_of_predictions = len(predictions)\n",
    "\n",
    "# intialise arrays for storing labels and classifications as words\n",
    "true_labels = []\n",
    "classification = []\n",
    "\n",
    "# loop through all predictions\n",
    "for i in range(no_of_predictions):  \n",
    "    # convert vector labels to word labels\n",
    "    new_true = vector_to_label(test_labels[i])\n",
    "    true_labels.append(new_true)\n",
    "    \n",
    "    # get index where vector equals one\n",
    "    # vector should be 1-hot encoded\n",
    "    label_no = np.argmax(predictions[i])\n",
    "    # output word classification to array\n",
    "    classification.append(label_names[label_no])\n",
    "    \n",
    "# check stats for classification vs ground truth for each instrument\n",
    "for label in label_names:\n",
    "    TP, FP, TN, FN = counters(true_labels, classification, label)\n",
    "\n",
    "    # print results\n",
    "    print(label, \"TP:\", TP)\n",
    "    print(label, \"FP:\",  FP)\n",
    "    print(label, \"TN:\", TN)\n",
    "    print(label, \"FN:\", FN)\n",
    "    print(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "528e7838-802c-4734-a8ba-f3f9b76a6027",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload test case audio\n",
    "test_audio = \"bensound-clapandyell.mp3\"\n",
    "case_study_snd, sr = librosa.load(test_audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "e0e89505-c325-4738-90a5-c48db81d3a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialise array for storing output audio\n",
    "audio = []\n",
    "\n",
    "# having some issues with that there sample rate\n",
    "# sr = 22050\n",
    "\n",
    "estimates = UMXpredict.separate(\n",
    "    torch.as_tensor(case_study_snd).float(),\n",
    "    rate = sr,\n",
    "    device = device\n",
    ")\n",
    "for target, estimates in estimates.items():\n",
    "    print(target)\n",
    "    output = estimates.detach().cpu().numpy()[0]\n",
    "    audio.append(output)\n",
    "    display(Audio(output, rate=sr*2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f083ebb7-ce6a-4837-a152-97f19276b262",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 : 58\n",
      "1 : 58\n",
      "2 : 58\n",
      "3 : 58\n",
      "Total no of spectrograms:  232\n"
     ]
    }
   ],
   "source": [
    "# calculate 3 second window\n",
    "three_seconds = 3 * sr\n",
    "\n",
    "# initialise array for storing case study spectrograms\n",
    "case_study_spectrograms = []\n",
    "\n",
    "# compute mel spec parameters\n",
    "win_len = int(0.05 * sr)\n",
    "hop_len = int(0.025 * sr)\n",
    "n_mels = 64\n",
    "\n",
    "# loop through each audio output from open-unmix\n",
    "for n, output in enumerate(audio):\n",
    "    \n",
    "    # iniitalise variabel for counting number of spectrograms per output\n",
    "    spec_counter = 0\n",
    "    \n",
    "    # convert output to mono audio\n",
    "    output_mono = librosa.to_mono(output)\n",
    "    \n",
    "    for i in range(len(snd)):\n",
    "        \n",
    "        # cut audio at three second windows\n",
    "        if (i % three_seconds) == 0:\n",
    "\n",
    "            # compute mel spectrogram from raw audio\n",
    "            spec = mel_spec(output_mono[i:i+three_seconds], sr=sr, n_mels=n_mels, hop_length=hop_len, \n",
    "                                                  win_length=win_len, window='hann')\n",
    "\n",
    "            # if spectrogram is smaller than network input, skip\n",
    "            if(spec.shape[1] < 121):\n",
    "                continue\n",
    "            \n",
    "            # rescale output range to be logarithmic\n",
    "            # corresponds to human perception\n",
    "            spec = tfio.audio.dbscale(spec, top_db=80)\n",
    "\n",
    "            # augment spectrogram in terms of frequency\n",
    "            augmented_spec = tfio.audio.freq_mask(spec, param=10)\n",
    "\n",
    "            # convert to original spectrogram to tensor\n",
    "            # add new axis to account for channels\n",
    "            spec = spec[..., tf.newaxis]\n",
    "            spec = tf.convert_to_tensor(spec)\n",
    "            # reshape to tensor shape\n",
    "            spec = tf.reshape(spec, (64, 121, 1))\n",
    "           \n",
    "            case_study_spectrograms.append(spec)\n",
    "            spec_counter += 1\n",
    "        \n",
    "    # count number of spectrograms per output\n",
    "    print(n, \":\", spec_counter)\n",
    "    \n",
    "print(\"Total no of spectrograms: \", len(case_study_spectrograms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a77ccec3-60d7-4c74-8605-b60da5f8110d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63.44747\n"
     ]
    }
   ],
   "source": [
    "# get largest value from across spectrograms\n",
    "case_study_max = np.amax(case_study_spectrograms)\n",
    "\n",
    "# normalise all specs according to maximum value\n",
    "normalised_case_study = np.abs(case_study_spectrograms // case_study_max)\n",
    "# if value in spectrogram is NAN, replace with 0\n",
    "normalised_case_study = np.where(np.isnan(normalised_case_study), 0, normalised_case_study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4a80f336-1ce0-4db1-845b-4717a1acfd28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use model to get label predictions\n",
    "case_study_predictions = model.predict(normalised_case_study)\n",
    "\n",
    "# initialise array for storing classifications\n",
    "case_study_classifications = []\n",
    "\n",
    "# loop through all spectrograms and get classification\n",
    "for k in range(len(case_study_predictions)):\n",
    "    case_label_no = np.argmax(predictions[k])\n",
    "    # output word classification to array\n",
    "    case_study_classifications.append(label_names[case_label_no])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ad14dee3-ae93-4155-8587-931fb96236e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flute\n",
      "Clarinet\n",
      "Sax\n",
      "Trumpet\n"
     ]
    }
   ],
   "source": [
    "# split classifications into groups of input source\n",
    "voice_class = case_study_classifications[:58]\n",
    "drums_class = case_study_classifications[58:116]\n",
    "bass_class = case_study_classifications[116:174]\n",
    "other_class = case_study_classifications[174:]\n",
    "\n",
    "# count classifications of each instrument per output\n",
    "unique_voice = np.unique(voice_class, return_counts=True)\n",
    "max_value_location = np.argmax(unique_voice[1])\n",
    "final_voice_classification = unique_voice[0][max_value_location]\n",
    "\n",
    "unique_drums = np.unique(drums_class, return_counts=True)\n",
    "max_value_location_d = np.argmax(unique_drums[1])\n",
    "final_drums_classification = unique_drums[0][max_value_location_d]\n",
    "\n",
    "unique_bass = np.unique(bass_class, return_counts=True)\n",
    "max_value_location_b = np.argmax(unique_bass[1])\n",
    "final_bass_classification = unique_bass[0][max_value_location_b]\n",
    "\n",
    "unique_other = np.unique(other_class, return_counts=True)\n",
    "max_value_location_o = np.argmax(unique_other[1])\n",
    "final_other_classification = unique_other[0][max_value_location_o]\n",
    "\n",
    "# print final output classification\n",
    "print(final_voice_classification)\n",
    "print(final_drums_classification)\n",
    "print(final_bass_classification)\n",
    "print(final_other_classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d10a96-30bc-4544-bd55-1952fc1d3d93",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
