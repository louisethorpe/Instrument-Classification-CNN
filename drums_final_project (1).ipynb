{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "adee6ef9-8529-43b1-b59e-43320951e4a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final project final code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b0c9be9-c671-42d8-9289-5640fece6f7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openunmix\n",
      "  Using cached openunmix-1.2.1-py3-none-any.whl (46 kB)\n",
      "Requirement already satisfied: torchaudio>=0.9.0 in /opt/conda/lib/python3.9/site-packages (from openunmix) (0.10.1+cu113)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.9/site-packages (from openunmix) (1.20.3)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.9/site-packages (from openunmix) (4.62.3)\n",
      "Requirement already satisfied: torch>=1.9.0 in /opt/conda/lib/python3.9/site-packages (from openunmix) (1.10.1+cu113)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.9/site-packages (from torch>=1.9.0->openunmix) (4.0.1)\n",
      "Installing collected packages: openunmix\n",
      "Successfully installed openunmix-1.2.1\n"
     ]
    }
   ],
   "source": [
    "!pip install openunmix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4cae7790-13ac-43e6-a466-4b17713fbef9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: soundfile in /opt/conda/lib/python3.9/site-packages (0.10.3.post1)\n",
      "Requirement already satisfied: cffi>=1.0 in /opt/conda/lib/python3.9/site-packages (from soundfile) (1.15.0)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.9/site-packages (from cffi>=1.0->soundfile) (2.21)\n"
     ]
    }
   ],
   "source": [
    "!pip install soundfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e58f727-a859-464d-bb99-2cebb63ce790",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Using cached tensorflow-2.8.0-cp39-cp39-manylinux2010_x86_64.whl (497.6 MB)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /opt/conda/lib/python3.9/site-packages (from tensorflow) (3.6.0)\n",
      "Collecting keras<2.9,>=2.8.0rc0\n",
      "  Using cached keras-2.8.0-py2.py3-none-any.whl (1.4 MB)\n",
      "Collecting tensorboard<2.9,>=2.8\n",
      "  Using cached tensorboard-2.8.0-py3-none-any.whl (5.8 MB)\n",
      "Collecting google-pasta>=0.1.1\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Collecting absl-py>=0.4.0\n",
      "  Using cached absl_py-1.0.0-py3-none-any.whl (126 kB)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.9/site-packages (from tensorflow) (1.16.0)\n",
      "Collecting gast>=0.2.1\n",
      "  Using cached gast-0.5.3-py3-none-any.whl (19 kB)\n",
      "Collecting astunparse>=1.6.0\n",
      "  Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting libclang>=9.0.1\n",
      "  Using cached libclang-13.0.0-py2.py3-none-manylinux1_x86_64.whl (14.5 MB)\n",
      "Requirement already satisfied: numpy>=1.20 in /opt/conda/lib/python3.9/site-packages (from tensorflow) (1.20.3)\n",
      "Collecting opt-einsum>=2.3.2\n",
      "  Using cached opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "Collecting keras-preprocessing>=1.1.1\n",
      "  Using cached Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in /opt/conda/lib/python3.9/site-packages (from tensorflow) (3.19.2)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.9/site-packages (from tensorflow) (4.0.1)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1\n",
      "  Using cached tensorflow_io_gcs_filesystem-0.24.0-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.1 MB)\n",
      "Collecting termcolor>=1.1.0\n",
      "  Using cached termcolor-1.1.0-py3-none-any.whl\n",
      "Collecting tf-estimator-nightly==2.8.0.dev2021122109\n",
      "  Using cached tf_estimator_nightly-2.8.0.dev2021122109-py2.py3-none-any.whl (462 kB)\n",
      "Collecting grpcio<2.0,>=1.24.3\n",
      "  Using cached grpcio-1.44.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.3 MB)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.9/site-packages (from tensorflow) (60.5.0)\n",
      "Collecting wrapt>=1.11.0\n",
      "  Using cached wrapt-1.14.0-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (77 kB)\n",
      "Collecting flatbuffers>=1.12\n",
      "  Using cached flatbuffers-2.0-py2.py3-none-any.whl (26 kB)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.9/site-packages (from astunparse>=1.6.0->tensorflow) (0.37.1)\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Using cached google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
      "Collecting werkzeug>=0.11.15\n",
      "  Using cached Werkzeug-2.1.1-py3-none-any.whl (224 kB)\n",
      "Collecting tensorboard-data-server<0.7.0,>=0.6.0\n",
      "  Using cached tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n",
      "Collecting tensorboard-plugin-wit>=1.6.0\n",
      "  Using cached tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.9/site-packages (from tensorboard<2.9,>=2.8->tensorflow) (2.27.1)\n",
      "Collecting google-auth<3,>=1.6.3\n",
      "  Using cached google_auth-2.6.3-py2.py3-none-any.whl (156 kB)\n",
      "Collecting markdown>=2.6.8\n",
      "  Using cached Markdown-3.3.6-py3-none-any.whl (97 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Using cached pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "Collecting cachetools<6.0,>=2.0.0\n",
      "  Using cached cachetools-5.0.0-py3-none-any.whl (9.1 kB)\n",
      "Collecting rsa<5,>=3.1.4\n",
      "  Using cached rsa-4.8-py3-none-any.whl (39 kB)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Using cached requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /opt/conda/lib/python3.9/site-packages (from markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow) (4.10.0)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (2.0.10)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (3.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (1.26.8)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.9/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow) (3.7.0)\n",
      "Collecting pyasn1<0.5.0,>=0.4.6\n",
      "  Using cached pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.9/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow) (3.1.1)\n",
      "Installing collected packages: pyasn1, rsa, pyasn1-modules, cachetools, requests-oauthlib, google-auth, werkzeug, tensorboard-plugin-wit, tensorboard-data-server, markdown, grpcio, google-auth-oauthlib, absl-py, wrapt, tf-estimator-nightly, termcolor, tensorflow-io-gcs-filesystem, tensorboard, opt-einsum, libclang, keras-preprocessing, keras, google-pasta, gast, flatbuffers, astunparse, tensorflow\n",
      "Successfully installed absl-py-1.0.0 astunparse-1.6.3 cachetools-5.0.0 flatbuffers-2.0 gast-0.5.3 google-auth-2.6.3 google-auth-oauthlib-0.4.6 google-pasta-0.2.0 grpcio-1.44.0 keras-2.8.0 keras-preprocessing-1.1.2 libclang-13.0.0 markdown-3.3.6 opt-einsum-3.3.0 pyasn1-0.4.8 pyasn1-modules-0.2.8 requests-oauthlib-1.3.1 rsa-4.8 tensorboard-2.8.0 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tensorflow-2.8.0 tensorflow-io-gcs-filesystem-0.24.0 termcolor-1.1.0 tf-estimator-nightly-2.8.0.dev2021122109 werkzeug-2.1.1 wrapt-1.14.0\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f71a73c-c4a8-48e5-971e-6e2eafe18419",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow_io\n",
      "  Using cached tensorflow_io-0.24.0-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (23.4 MB)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem==0.24.0 in /opt/conda/lib/python3.9/site-packages (from tensorflow_io) (0.24.0)\n",
      "Installing collected packages: tensorflow-io\n",
      "Successfully installed tensorflow-io-0.24.0\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow_io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a371d708-4909-433b-8037-e6dce7a5263b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dependencies\n",
    "\n",
    "import torch\n",
    "import torchaudio\n",
    "import tensorflow as tf\n",
    "import tensorflow_io as tfio\n",
    "\n",
    "import numpy as np\n",
    "import scipy\n",
    "import os\n",
    "from IPython.display import Audio, display\n",
    "\n",
    "import librosa\n",
    "import sklearn\n",
    "import math\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as nnF\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "import soundfile\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "from librosa.feature import melspectrogram as mel_spec\n",
    "\n",
    "from openunmix import predict\n",
    "\n",
    "# set to GPU if available\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f6883fb5-a923-4fc7-9756-ab70faba34b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN/BUILD CNN\n",
    "\n",
    "# path to training data \n",
    "train_ds  = os.path.expanduser(\"~/teaching_material/own_work/train_data\")\n",
    "\n",
    "# path to drums training data\n",
    "drum_ds  = os.path.expanduser(\"~/drum_data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "507a568b-0f87-4f23-8305-42f4555e5816",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to shuffle two arrays\n",
    "def shuffled_arrays(x, y):\n",
    "    p = np.random.permutation(x.shape[0])\n",
    "    return x[p], y[p]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3a0347f2-cdce-4d04-bd05-19e9c19520bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to convert training data into spectrograms\n",
    "def convert_audio_to_spectrogram(infile, win_size = 0.05, hop_size = 0.025, n_mels = 64):\n",
    "   \n",
    "    # load audio file and get sample rate\n",
    "    snd, fs = librosa.load(infile)\n",
    "    \n",
    "    # compute mel spec parameters\n",
    "    win_len = int(win_size * fs)\n",
    "    hop_len = int(hop_size * fs)\n",
    "    \n",
    "    # compute mel spectrogram from raw audio\n",
    "    spec = mel_spec(snd, sr=fs, n_mels=n_mels, hop_length=hop_len, \n",
    "                                          win_length=win_len, window='hann')\n",
    "    \n",
    "    # rescale output range to be logarithmic\n",
    "    # corresponds to human perception\n",
    "    spec = tfio.audio.dbscale(spec, top_db=80)\n",
    "    \n",
    "    # augment spectrogram in terms of frequency\n",
    "    augmented_spec = tfio.audio.freq_mask(spec, param=10)\n",
    "    \n",
    "    # convert to original spectrogram to tensor\n",
    "    # add new axis to account for channels\n",
    "    spec = spec[..., tf.newaxis]\n",
    "    spec = tf.convert_to_tensor(spec)\n",
    "    # reshape to tensor shape\n",
    "    spec = tf.reshape(spec, (64, 121, 1))\n",
    "    \n",
    "    # convert to augmented spectrogram to tensor\n",
    "    augmented_spec = augmented_spec[..., tf.newaxis]\n",
    "    augmented_spec = tf.convert_to_tensor(augmented_spec)\n",
    "    augmented_spec = tf.reshape(augmented_spec, (64, 121, 1))\n",
    "    \n",
    "    # output both\n",
    "    return spec, augmented_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d85f0949-0b48-4412-96ed-45b7b8162d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to get training data labels from file name\n",
    "def get_label(infile):\n",
    "    \n",
    "    # get section of file name associated with instrument\n",
    "    file_name = infile[1:4] \n",
    "\n",
    "    if file_name == 'cel':\n",
    "        label = [1,0,0,0,0,0,0,0,0,0,0,0,0]\n",
    "        \n",
    "    elif file_name == 'cla':\n",
    "        label = [0,1,0,0,0,0,0,0,0,0,0,0,0]\n",
    "        \n",
    "    elif file_name == 'flu':\n",
    "        label = [0,0,1,0,0,0,0,0,0,0,0,0,0]\n",
    "        \n",
    "    elif file_name == 'gac':\n",
    "        label = [0,0,0,1,0,0,0,0,0,0,0,0,0]\n",
    "        \n",
    "    elif file_name == 'gel':\n",
    "        label = [0,0,0,0,1,0,0,0,0,0,0,0,0]\n",
    "        \n",
    "    elif file_name == 'org':\n",
    "        label = [0,0,0,0,0,1,0,0,0,0,0,0,0]\n",
    "        \n",
    "    elif file_name == 'pia':\n",
    "        label = [0,0,0,0,0,0,1,0,0,0,0,0,0]\n",
    "        \n",
    "    elif file_name == 'sax':\n",
    "        label = [0,0,0,0,0,0,0,1,0,0,0,0,0]\n",
    "        \n",
    "    elif file_name == 'tru':\n",
    "        label = [0,0,0,0,0,0,0,0,1,0,0,0,0]\n",
    "        \n",
    "    elif file_name == 'vio':\n",
    "        label = [0,0,0,0,0,0,0,0,0,1,0,0,0]\n",
    "        \n",
    "    elif file_name == 'voi':\n",
    "        label = [0,0,0,0,0,0,0,0,0,0,1,0,0]\n",
    "        \n",
    "    else:\n",
    "        label = [0,0,0,0,0,0,0,0,0,0,0,0,1]\n",
    "    \n",
    "    return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "34dc018b-b27d-4dbc-b296-5990cd7b84cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# store labels names as strings in an array for output use\n",
    "label_names = ['Cello', 'Clarinet', 'Flute', 'Acoustic Guitar', 'Electric Guitar', \n",
    "               'Organ', 'Piano', 'Sax', 'Trumpet', 'Violin', 'Voice', 'Drums', 'Other']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "392e86fb-d59e-4dd2-b9dc-d5877f368a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to convert vectorised labels back to words\n",
    "def vector_to_label(vector):\n",
    "    \n",
    "    word = \"Undefined\"\n",
    "    \n",
    "    if vector[0] == 1:\n",
    "        word = \"Cello\"\n",
    "        \n",
    "    elif vector[1] == 1:\n",
    "        word = \"Clarinet\"\n",
    "        \n",
    "    elif vector[2] == 1:\n",
    "        word = \"Flute\"\n",
    "            \n",
    "    elif vector[3] == 1:\n",
    "        word = \"Acoustic Guitar\"\n",
    "                \n",
    "    elif vector[4] == 1:\n",
    "        word = \"Electric Guitar\"\n",
    "                    \n",
    "    elif vector[5] == 1:\n",
    "        word = \"Organ\"\n",
    "                        \n",
    "    elif vector[6] == 1:\n",
    "        word = \"Piano\"\n",
    "                            \n",
    "    elif vector[7] == 1:\n",
    "        word = \"Sax\"\n",
    "                                \n",
    "    elif vector[8] == 1:\n",
    "        word = \"Trumpet\"\n",
    "                                    \n",
    "    elif vector[9] == 1:\n",
    "        word = \"Violin\"\n",
    "                                        \n",
    "    elif vector[10] == 1:\n",
    "        word = \"Voice\"\n",
    "        \n",
    "    elif vector[11] == 1:\n",
    "        word = \"Drums\"\n",
    "\n",
    "    return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8e7b2f09-9150-4375-844b-06f2d8a04f7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3261\n"
     ]
    }
   ],
   "source": [
    "# check total number of training samples\n",
    "no_of_entries = len([name for name in os.listdir(train_ds)])\n",
    "print(no_of_entries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1bb168dc-393c-479d-a3ac-0652602fda1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-12 14:04:51.204164: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-04-12 14:05:10.629022: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 41734 MB memory:  -> device: 0, name: NVIDIA A40, pci bus id: 0000:17:00.0, compute capability: 8.6\n",
      "2022-04-12 14:05:10.630817: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 41734 MB memory:  -> device: 1, name: NVIDIA A40, pci bus id: 0000:65:00.0, compute capability: 8.6\n",
      "2022-04-12 14:05:10.632516: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 41734 MB memory:  -> device: 2, name: NVIDIA A40, pci bus id: 0000:ca:00.0, compute capability: 8.6\n",
      "2022-04-12 14:05:10.634169: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:3 with 41734 MB memory:  -> device: 3, name: NVIDIA A40, pci bus id: 0000:e3:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n",
      "100\n",
      "150\n",
      "200\n",
      "250\n",
      "300\n",
      "350\n",
      "400\n",
      "450\n",
      "500\n",
      "550\n",
      "600\n",
      "650\n",
      "700\n",
      "750\n",
      "800\n",
      "850\n",
      "900\n",
      "950\n",
      "1000\n",
      "1050\n",
      "1100\n",
      "1150\n",
      "1200\n",
      "1250\n",
      "1300\n",
      "1350\n",
      "1400\n",
      "1450\n",
      "1500\n",
      "1550\n",
      "1600\n",
      "1650\n",
      "1700\n",
      "1750\n",
      "1800\n",
      "1850\n",
      "1900\n",
      "1950\n",
      "2000\n",
      "2050\n",
      "2100\n",
      "2150\n",
      "2200\n",
      "2250\n",
      "2300\n",
      "2350\n",
      "2400\n",
      "2450\n",
      "2500\n",
      "2550\n",
      "2600\n",
      "2650\n",
      "2700\n",
      "2750\n"
     ]
    }
   ],
   "source": [
    "# initialise an array for storing output spectrogram tensors\n",
    "spectrograms = np.zeros((no_of_entries * 2, 64, 121, 1))\n",
    "\n",
    "# initialise counter for counting number of spectrograms calculated\n",
    "# also used for location for storing spectrograms\n",
    "counter = 0\n",
    "\n",
    "# scan through training data directory and compute spectrograms\n",
    "for entry in os.scandir(train_ds):\n",
    "    \n",
    "    # if file is not .wav then skip\n",
    "    if str(os.listdir(train_ds)[counter][-3:]) != 'wav':\n",
    "        continue\n",
    "    \n",
    "    # add output spectrograms to spectrogram array\n",
    "    spec, augmented_spec = convert_audio_to_spectrogram(entry)\n",
    "    spectrograms[counter] = spec\n",
    "    spectrograms[counter + 1] = augmented_spec\n",
    "    # advance counter\n",
    "    counter += 2\n",
    "    \n",
    "    # print progress\n",
    "    if counter % 50 == 0:\n",
    "        print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bfe887c7-0826-4453-bdec-e2fa6bfcefaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initalise array for storing label data\n",
    "labels = np.zeros((no_of_entries * 2, 13))\n",
    "\n",
    "# counter for inputting labels in array\n",
    "label_counter = 0\n",
    "\n",
    "for name in os.listdir(train_ds):\n",
    "    \n",
    "    # skip file if not .wav\n",
    "    if str(os.listdir(train_ds)[label_counter][-3:]) != 'wav':\n",
    "        continue\n",
    "    \n",
    "    # output labels to array\n",
    "    labels[label_counter] = get_label(name)\n",
    "    labels[label_counter + 1] = get_label(name)    \n",
    "    # advance counter\n",
    "    label_counter += 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8037585e-7aa4-4048-bb1d-699c3895cc7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get spectrograms and label arrays to correct length\n",
    "spectrograms = spectrograms[:counter]\n",
    "labels = labels[:label_counter]\n",
    "\n",
    "# get total number of samples from spectrogram counter\n",
    "total_length = counter\n",
    "\n",
    "# print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6f50a3e4-96fc-48a7-bb82-0234d530ab34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/librosa/core/spectrum.py:222: UserWarning: n_fft=2048 is too small for input signal of length=184\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "520\n"
     ]
    }
   ],
   "source": [
    "drum_spectrograms = []\n",
    "\n",
    "for drum_file in os.scandir(drum_ds):\n",
    "    \n",
    "    spec_counter = 0\n",
    "    \n",
    "    snd, sr = librosa.load(drum_file)\n",
    "    \n",
    "    three_seconds = 3 * sr\n",
    "    n_mels = 64\n",
    "    win_len = int(0.05 * sr)\n",
    "    hop_len = int(0.025 * sr)\n",
    "    \n",
    "    for i in range(len(snd)):\n",
    "        \n",
    "        if spec_counter > 13:\n",
    "            break\n",
    "        \n",
    "        if (i % three_seconds) == 0:\n",
    "\n",
    "            # compute mel spectrogram from raw audio\n",
    "            spec = mel_spec(snd[i:i+three_seconds], sr=sr, n_mels=n_mels, hop_length=hop_len, \n",
    "                                                  win_length=win_len, window='hann')\n",
    "            \n",
    "            spec_counter += 1\n",
    "\n",
    "            if(spec.shape[1] < 121):\n",
    "                continue\n",
    "            \n",
    "            # rescale output range to be logarithmic\n",
    "            # corresponds to human perception\n",
    "            spec = tfio.audio.dbscale(spec, top_db=80)\n",
    "\n",
    "            # augment spectrogram in terms of frequency\n",
    "            augmented_spec = tfio.audio.freq_mask(spec, param=10)\n",
    "\n",
    "            # convert to original spectrogram to tensor\n",
    "            # add new axis to account for channels\n",
    "            spec = spec[..., tf.newaxis]\n",
    "            spec = tf.convert_to_tensor(spec)\n",
    "            # reshape to tensor shape\n",
    "            spec = tf.reshape(spec, (64, 121, 1))\n",
    "            \n",
    "            # convert to augmented spectrogram to tensor\n",
    "            augmented_spec = augmented_spec[..., tf.newaxis]\n",
    "            augmented_spec = tf.convert_to_tensor(augmented_spec)\n",
    "            augmented_spec = tf.reshape(augmented_spec, (64, 121, 1))\n",
    "           \n",
    "            drum_spectrograms.append(spec)\n",
    "            drum_spectrograms.append(augmented_spec)\n",
    "            \n",
    "            \n",
    "print(len(drum_spectrograms))\n",
    "\n",
    "# drum_spectrograms = drum_spectrograms[:300]\n",
    "\n",
    "# print(len(drum_spectrograms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c0c7abf5-5902-4f1b-ae03-0cdff5880eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "drum_spectrograms = drum_spectrograms[:250]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8f967026-2e06-4d73-8fec-df0ff36335a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(drum_spectrograms.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a7e2aacc-5ec7-4e0e-8ec5-ef140d6fc5b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n"
     ]
    }
   ],
   "source": [
    "drum_labels = np.zeros((len(drum_spectrograms), 13))\n",
    "\n",
    "for i in range(len(drum_spectrograms)):\n",
    "    drum_labels[i] = [0,0,0,0,0,0,0,0,0,0,0,1,0]\n",
    "    \n",
    "print(drum_labels[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "802dd0e2-ab3a-4080-90b8-732924362e61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66.93445587158203\n",
      "67.121185\n"
     ]
    }
   ],
   "source": [
    "# get largest value from across spectrograms\n",
    "most_max_regular = np.amax(spectrograms)\n",
    "\n",
    "print(most_max_regular)\n",
    "\n",
    "# normalise all specs according to maximum value\n",
    "# maybe i will take away the floor dividing\n",
    "normalised_specs = np.abs(spectrograms // most_max_regular)\n",
    "normalised_specs = np.where(np.isnan(normalised_specs), 0, normalised_specs)\n",
    "\n",
    "# same for drums\n",
    "most_max_drums = np.amax(drum_spectrograms)\n",
    "\n",
    "print(most_max_drums)\n",
    "\n",
    "# if np.any(np.isnan(drum_spectrograms)):\n",
    "#     print(\"bruh\")\n",
    "\n",
    "# normalise all specs according to maximum value\n",
    "normalised_drum_specs = np.abs(drum_spectrograms // most_max_drums)\n",
    "normalised_drum_specs = np.where(np.isnan(normalised_drum_specs), 0, normalised_drum_specs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "00eae109-5787-4a9e-99b7-4ad746417829",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3044, 64, 121, 1)\n",
      "(3044, 13)\n"
     ]
    }
   ],
   "source": [
    "# TO DO :\n",
    "\n",
    "all_spectrograms = [normalised_specs, normalised_drum_specs]\n",
    "all_spectrograms = np.concatenate(all_spectrograms)\n",
    "all_labels = [labels, drum_labels]\n",
    "all_labels = np.concatenate(all_labels)\n",
    "\n",
    "print(all_spectrograms.shape)\n",
    "print(all_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "27017baa-8f4e-4add-9057-f7c83510c619",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle spectrograms and labels with same permutation \n",
    "suffled_spectrograms, shuffled_labels = shuffled_arrays(all_spectrograms, all_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c4a98499-5e0b-4f4e-9333-df0ee93f9eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into train and test\n",
    "# make train data 80% of all data\n",
    "train_specs = suffled_spectrograms[:int(total_length * 0.8)]\n",
    "\n",
    "# val data is 10%\n",
    "val_specs = suffled_spectrograms[int(total_length * 0.8):int(total_length * 0.9)]\n",
    "\n",
    "# test data is the last 10%\n",
    "test_specs = suffled_spectrograms[int(total_length * 0.9):total_length]\n",
    "\n",
    "# split labels same way as spectrograms\n",
    "train_labels = shuffled_labels[:int(total_length * 0.8)]\n",
    "val_labels = shuffled_labels[int(total_length * 0.8):int(total_length * 0.9)]\n",
    "test_labels = shuffled_labels[int(total_length * 0.9):total_length]\n",
    "\n",
    "# convert spectrograms into tensor for training\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(train_specs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d8c81833-8e52-450b-b22d-45fec194ac1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 63, 120, 32)       160       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 31, 60, 32)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 30, 59, 64)        8256      \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 15, 29, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 14, 28, 128)       32896     \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 7, 14, 128)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 7, 14, 128)       512       \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 6, 13, 128)        65664     \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 9984)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1024)              10224640  \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 512)               524800    \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 13)                6669      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 10,863,597\n",
      "Trainable params: 10,863,341\n",
      "Non-trainable params: 256\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# BUILD CNN MODEL\n",
    "\n",
    "# use He initialisation\n",
    "initializer = tf.keras.initializers.HeNormal()\n",
    "\n",
    "# set up model as sequential keras model\n",
    "model = models.Sequential()\n",
    "\n",
    "# add convolutional layer followed by max pooling\n",
    "model.add(layers.Conv2D(32, (2, 2), activation='tanh', input_shape=(64, 121, 1), kernel_initializer=initializer))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "# add convolutional layer followed by max pooling\n",
    "model.add(layers.Conv2D(64, (2, 2), activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "# add convolutional layer followed by max pooling\n",
    "model.add(layers.Conv2D(128, (2, 2), activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "# normalise the batch \n",
    "model.add(layers.BatchNormalization(momentum = 0.9))\n",
    "\n",
    "# add convolutional layer - this is ususally 16 not 128\n",
    "model.add(layers.Conv2D(128, (2, 2), activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)))\n",
    "\n",
    "# flatten\n",
    "model.add(layers.Flatten())\n",
    "\n",
    "# three dense layers in sequence to 12 possibel outputs\n",
    "# model.add(layers.Dense(128, activation='tanh', kernel_regularizer=tf.keras.regularizers.l2(0.001)))\n",
    "# model.add(layers.Dense(64, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)))\n",
    "model.add(layers.Dense(1024, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)))\n",
    "model.add(layers.Dense(512, activation='tanh', kernel_regularizer=tf.keras.regularizers.l2(0.001)))\n",
    "model.add(layers.Dense(13, activation='softmax'))\n",
    "\n",
    "# summarise models\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b66e8fc1-6f83-4008-9ace-76e863f36414",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function to perform step down learning rate decay\n",
    "\n",
    "# code taken from stack overflow: REFERENCE\n",
    "\n",
    "def step_decay(epoch):\n",
    "    initial_lrate = 0.00001\n",
    "    drop = 0.5\n",
    "    epochs_drop = 50.0\n",
    "    lrate = initial_lrate * math.pow(drop, math.floor((1+epoch)/epochs_drop))\n",
    "    return lrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e4e8a0be-ec20-4aa9-8e4e-a6604f9b8f82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: LearningRateScheduler setting learning rate to 1e-05.\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-12 14:15:45.692939: I tensorflow/stream_executor/cuda/cuda_dnn.cc:368] Loaded cuDNN version 8204\n",
      "2022-04-12 14:15:47.480993: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-04-12 14:15:47.482626: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-04-12 14:15:47.482694: W tensorflow/stream_executor/gpu/asm_compiler.cc:80] Couldn't get ptxas version string: INTERNAL: Couldn't invoke ptxas --version\n",
      "2022-04-12 14:15:47.484377: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-04-12 14:15:47.484535: W tensorflow/stream_executor/gpu/redzone_allocator.cc:314] INTERNAL: Failed to launch ptxas\n",
      "Relying on driver to perform ptx compilation. \n",
      "Modify $PATH to customize ptxas location.\n",
      "This message will be only logged once.\n",
      "2022-04-12 14:15:48.791892: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70/70 [==============================] - 7s 21ms/step - loss: 5.0812 - accuracy: 0.2116 - val_loss: 4.8575 - val_accuracy: 0.3154 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 2: LearningRateScheduler setting learning rate to 1e-05.\n",
      "Epoch 2/100\n",
      "70/70 [==============================] - 1s 17ms/step - loss: 4.6920 - accuracy: 0.3826 - val_loss: 4.6520 - val_accuracy: 0.4050 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 3: LearningRateScheduler setting learning rate to 1e-05.\n",
      "Epoch 3/100\n",
      "70/70 [==============================] - 1s 17ms/step - loss: 4.4543 - accuracy: 0.5025 - val_loss: 4.5745 - val_accuracy: 0.4444 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 4: LearningRateScheduler setting learning rate to 1e-05.\n",
      "Epoch 4/100\n",
      "70/70 [==============================] - 1s 18ms/step - loss: 4.2522 - accuracy: 0.5928 - val_loss: 4.4410 - val_accuracy: 0.4624 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 5: LearningRateScheduler setting learning rate to 1e-05.\n",
      "Epoch 5/100\n",
      "70/70 [==============================] - 1s 18ms/step - loss: 4.0777 - accuracy: 0.6617 - val_loss: 4.3777 - val_accuracy: 0.4803 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 6: LearningRateScheduler setting learning rate to 1e-05.\n",
      "Epoch 6/100\n",
      "70/70 [==============================] - 1s 18ms/step - loss: 3.9208 - accuracy: 0.7311 - val_loss: 4.2578 - val_accuracy: 0.5556 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 7: LearningRateScheduler setting learning rate to 1e-05.\n",
      "Epoch 7/100\n",
      "70/70 [==============================] - 1s 19ms/step - loss: 3.7773 - accuracy: 0.7888 - val_loss: 4.1879 - val_accuracy: 0.5591 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 8: LearningRateScheduler setting learning rate to 1e-05.\n",
      "Epoch 8/100\n",
      "70/70 [==============================] - 1s 18ms/step - loss: 3.6502 - accuracy: 0.8246 - val_loss: 4.0856 - val_accuracy: 0.5986 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 9: LearningRateScheduler setting learning rate to 1e-05.\n",
      "Epoch 9/100\n",
      "70/70 [==============================] - 1s 17ms/step - loss: 3.5305 - accuracy: 0.8582 - val_loss: 4.0654 - val_accuracy: 0.6201 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 10: LearningRateScheduler setting learning rate to 1e-05.\n",
      "Epoch 10/100\n",
      "70/70 [==============================] - 1s 16ms/step - loss: 3.4242 - accuracy: 0.8949 - val_loss: 3.9512 - val_accuracy: 0.6559 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 11: LearningRateScheduler setting learning rate to 1e-05.\n",
      "Epoch 11/100\n",
      "70/70 [==============================] - 1s 16ms/step - loss: 3.3318 - accuracy: 0.9119 - val_loss: 3.8921 - val_accuracy: 0.6631 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 12: LearningRateScheduler setting learning rate to 1e-05.\n",
      "Epoch 12/100\n",
      "70/70 [==============================] - 1s 16ms/step - loss: 3.2423 - accuracy: 0.9324 - val_loss: 3.8136 - val_accuracy: 0.6846 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 13: LearningRateScheduler setting learning rate to 1e-05.\n",
      "Epoch 13/100\n",
      "70/70 [==============================] - 1s 17ms/step - loss: 3.1684 - accuracy: 0.9423 - val_loss: 3.7637 - val_accuracy: 0.7204 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 14: LearningRateScheduler setting learning rate to 1e-05.\n",
      "Epoch 14/100\n",
      "70/70 [==============================] - 1s 17ms/step - loss: 3.0969 - accuracy: 0.9548 - val_loss: 3.6923 - val_accuracy: 0.7276 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 15: LearningRateScheduler setting learning rate to 1e-05.\n",
      "Epoch 15/100\n",
      "70/70 [==============================] - 1s 18ms/step - loss: 3.0337 - accuracy: 0.9647 - val_loss: 3.6484 - val_accuracy: 0.7348 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 16: LearningRateScheduler setting learning rate to 1e-05.\n",
      "Epoch 16/100\n",
      "70/70 [==============================] - 1s 19ms/step - loss: 2.9800 - accuracy: 0.9696 - val_loss: 3.5794 - val_accuracy: 0.7312 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 17: LearningRateScheduler setting learning rate to 1e-05.\n",
      "Epoch 17/100\n",
      "70/70 [==============================] - 1s 18ms/step - loss: 2.9294 - accuracy: 0.9749 - val_loss: 3.5481 - val_accuracy: 0.7455 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 18: LearningRateScheduler setting learning rate to 1e-05.\n",
      "Epoch 18/100\n",
      "70/70 [==============================] - 1s 18ms/step - loss: 2.8837 - accuracy: 0.9826 - val_loss: 3.4840 - val_accuracy: 0.7491 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 19: LearningRateScheduler setting learning rate to 1e-05.\n",
      "Epoch 19/100\n",
      "70/70 [==============================] - 1s 18ms/step - loss: 2.8416 - accuracy: 0.9852 - val_loss: 3.4113 - val_accuracy: 0.7921 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 20: LearningRateScheduler setting learning rate to 1e-05.\n",
      "Epoch 20/100\n",
      "70/70 [==============================] - 1s 18ms/step - loss: 2.8027 - accuracy: 0.9897 - val_loss: 3.4044 - val_accuracy: 0.7599 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 21: LearningRateScheduler setting learning rate to 1e-05.\n",
      "Epoch 21/100\n",
      "70/70 [==============================] - 1s 18ms/step - loss: 2.7684 - accuracy: 0.9933 - val_loss: 3.3682 - val_accuracy: 0.7742 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 22: LearningRateScheduler setting learning rate to 1e-05.\n",
      "Epoch 22/100\n",
      "70/70 [==============================] - 1s 19ms/step - loss: 2.7371 - accuracy: 0.9946 - val_loss: 3.3129 - val_accuracy: 0.7993 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 23: LearningRateScheduler setting learning rate to 1e-05.\n",
      "Epoch 23/100\n",
      "70/70 [==============================] - 1s 19ms/step - loss: 2.7069 - accuracy: 0.9951 - val_loss: 3.3150 - val_accuracy: 0.7849 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 24: LearningRateScheduler setting learning rate to 1e-05.\n",
      "Epoch 24/100\n",
      "70/70 [==============================] - 1s 17ms/step - loss: 2.6803 - accuracy: 0.9960 - val_loss: 3.2395 - val_accuracy: 0.8136 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 25: LearningRateScheduler setting learning rate to 1e-05.\n",
      "Epoch 25/100\n",
      "70/70 [==============================] - 1s 17ms/step - loss: 2.6521 - accuracy: 0.9964 - val_loss: 3.2071 - val_accuracy: 0.8172 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 26: LearningRateScheduler setting learning rate to 1e-05.\n",
      "Epoch 26/100\n",
      "70/70 [==============================] - 1s 18ms/step - loss: 2.6270 - accuracy: 0.9982 - val_loss: 3.1736 - val_accuracy: 0.8280 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 27: LearningRateScheduler setting learning rate to 1e-05.\n",
      "Epoch 27/100\n",
      "70/70 [==============================] - 1s 18ms/step - loss: 2.6024 - accuracy: 0.9987 - val_loss: 3.1409 - val_accuracy: 0.8387 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 28: LearningRateScheduler setting learning rate to 1e-05.\n",
      "Epoch 28/100\n",
      "70/70 [==============================] - 1s 18ms/step - loss: 2.5801 - accuracy: 0.9987 - val_loss: 3.1234 - val_accuracy: 0.8315 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 29: LearningRateScheduler setting learning rate to 1e-05.\n",
      "Epoch 29/100\n",
      "70/70 [==============================] - 4s 61ms/step - loss: 2.5569 - accuracy: 0.9982 - val_loss: 3.0908 - val_accuracy: 0.8280 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 30: LearningRateScheduler setting learning rate to 1e-05.\n",
      "Epoch 30/100\n",
      "70/70 [==============================] - 1s 18ms/step - loss: 2.5345 - accuracy: 0.9987 - val_loss: 3.0640 - val_accuracy: 0.8423 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 31: LearningRateScheduler setting learning rate to 1e-05.\n",
      "Epoch 31/100\n",
      "70/70 [==============================] - 1s 17ms/step - loss: 2.5124 - accuracy: 0.9991 - val_loss: 3.0767 - val_accuracy: 0.8244 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 32: LearningRateScheduler setting learning rate to 1e-05.\n",
      "Epoch 32/100\n",
      "70/70 [==============================] - 1s 18ms/step - loss: 2.4901 - accuracy: 1.0000 - val_loss: 3.0276 - val_accuracy: 0.8244 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 33: LearningRateScheduler setting learning rate to 1e-05.\n",
      "Epoch 33/100\n",
      "70/70 [==============================] - 1s 17ms/step - loss: 2.4684 - accuracy: 0.9991 - val_loss: 3.0065 - val_accuracy: 0.8280 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 34: LearningRateScheduler setting learning rate to 1e-05.\n",
      "Epoch 34/100\n",
      "70/70 [==============================] - 1s 17ms/step - loss: 2.4471 - accuracy: 1.0000 - val_loss: 2.9707 - val_accuracy: 0.8387 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 35: LearningRateScheduler setting learning rate to 1e-05.\n",
      "Epoch 35/100\n",
      "70/70 [==============================] - 1s 18ms/step - loss: 2.4257 - accuracy: 1.0000 - val_loss: 2.9505 - val_accuracy: 0.8387 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 36: LearningRateScheduler setting learning rate to 1e-05.\n",
      "Epoch 36/100\n",
      "70/70 [==============================] - 1s 17ms/step - loss: 2.4046 - accuracy: 1.0000 - val_loss: 2.9193 - val_accuracy: 0.8351 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 37: LearningRateScheduler setting learning rate to 1e-05.\n",
      "Epoch 37/100\n",
      "70/70 [==============================] - 1s 18ms/step - loss: 2.3837 - accuracy: 1.0000 - val_loss: 2.8999 - val_accuracy: 0.8315 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 38: LearningRateScheduler setting learning rate to 1e-05.\n",
      "Epoch 38/100\n",
      "70/70 [==============================] - 1s 18ms/step - loss: 2.3623 - accuracy: 1.0000 - val_loss: 2.8847 - val_accuracy: 0.8315 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 39: LearningRateScheduler setting learning rate to 1e-05.\n",
      "Epoch 39/100\n",
      "70/70 [==============================] - 1s 18ms/step - loss: 2.3414 - accuracy: 0.9996 - val_loss: 2.8546 - val_accuracy: 0.8351 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 40: LearningRateScheduler setting learning rate to 1e-05.\n",
      "Epoch 40/100\n",
      "70/70 [==============================] - 1s 15ms/step - loss: 2.3208 - accuracy: 1.0000 - val_loss: 2.8345 - val_accuracy: 0.8387 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 41: LearningRateScheduler setting learning rate to 1e-05.\n",
      "Epoch 41/100\n",
      "70/70 [==============================] - 3s 41ms/step - loss: 2.3001 - accuracy: 1.0000 - val_loss: 2.8173 - val_accuracy: 0.8495 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 42: LearningRateScheduler setting learning rate to 1e-05.\n",
      "Epoch 42/100\n",
      "70/70 [==============================] - 1s 17ms/step - loss: 2.2797 - accuracy: 1.0000 - val_loss: 2.7868 - val_accuracy: 0.8530 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 43: LearningRateScheduler setting learning rate to 1e-05.\n",
      "Epoch 43/100\n",
      "70/70 [==============================] - 1s 18ms/step - loss: 2.2598 - accuracy: 1.0000 - val_loss: 2.7748 - val_accuracy: 0.8495 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 44: LearningRateScheduler setting learning rate to 1e-05.\n",
      "Epoch 44/100\n",
      "70/70 [==============================] - 1s 18ms/step - loss: 2.2394 - accuracy: 1.0000 - val_loss: 2.7508 - val_accuracy: 0.8423 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 45: LearningRateScheduler setting learning rate to 1e-05.\n",
      "Epoch 45/100\n",
      "70/70 [==============================] - 1s 17ms/step - loss: 2.2192 - accuracy: 1.0000 - val_loss: 2.7331 - val_accuracy: 0.8459 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 46: LearningRateScheduler setting learning rate to 1e-05.\n",
      "Epoch 46/100\n",
      "70/70 [==============================] - 1s 18ms/step - loss: 2.1996 - accuracy: 1.0000 - val_loss: 2.7028 - val_accuracy: 0.8423 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 47: LearningRateScheduler setting learning rate to 1e-05.\n",
      "Epoch 47/100\n",
      "70/70 [==============================] - 3s 37ms/step - loss: 2.1797 - accuracy: 1.0000 - val_loss: 2.6851 - val_accuracy: 0.8495 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 48: LearningRateScheduler setting learning rate to 1e-05.\n",
      "Epoch 48/100\n",
      "70/70 [==============================] - 1s 18ms/step - loss: 2.1602 - accuracy: 1.0000 - val_loss: 2.6567 - val_accuracy: 0.8495 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 49: LearningRateScheduler setting learning rate to 1e-05.\n",
      "Epoch 49/100\n",
      "70/70 [==============================] - 1s 18ms/step - loss: 2.1406 - accuracy: 1.0000 - val_loss: 2.6387 - val_accuracy: 0.8495 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 50: LearningRateScheduler setting learning rate to 5e-06.\n",
      "Epoch 50/100\n",
      "70/70 [==============================] - 2s 28ms/step - loss: 2.1258 - accuracy: 1.0000 - val_loss: 2.6338 - val_accuracy: 0.8530 - lr: 5.0000e-06\n",
      "\n",
      "Epoch 51: LearningRateScheduler setting learning rate to 5e-06.\n",
      "Epoch 51/100\n",
      "70/70 [==============================] - 2s 23ms/step - loss: 2.1161 - accuracy: 1.0000 - val_loss: 2.6232 - val_accuracy: 0.8459 - lr: 5.0000e-06\n",
      "\n",
      "Epoch 52: LearningRateScheduler setting learning rate to 5e-06.\n",
      "Epoch 52/100\n",
      "70/70 [==============================] - ETA: 0s - loss: 2.1065 - accuracy: 1.0000"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-12 14:17:04.449800: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at save_restore_v2_ops.cc:160 : RESOURCE_EXHAUSTED: training_checkpoints/ckpt_52_temp/part-00000-of-00001.data-00000-of-00001.tempstate9833815605142357114; No space left on device\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "training_checkpoints/ckpt_52_temp/part-00000-of-00001.data-00000-of-00001.tempstate9833815605142357114; No space left on device [Op:SaveV2]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_93/1914695183.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m# train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m history = model.fit(train_specs, train_labels, epochs=100, callbacks=callbacks,\n\u001b[0m\u001b[1;32m     27\u001b[0m                    validation_data=(val_specs, val_labels))\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     55\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     56\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: training_checkpoints/ckpt_52_temp/part-00000-of-00001.data-00000-of-00001.tempstate9833815605142357114; No space left on device [Op:SaveV2]"
     ]
    }
   ],
   "source": [
    "# TRAIN THE NETWORK\n",
    "\n",
    "# initialise optimizer\n",
    "optimizer = tf.keras.optimizers.RMSprop(learning_rate=0.0001, momentum = 0.1)\n",
    "# optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "\n",
    "# compile the model\n",
    "model.compile(optimizer=optimizer,\n",
    "              loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "              metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# train the model using step down learning rate decay\n",
    "callbacks = [\n",
    "             tf.keras.callbacks.ModelCheckpoint(\n",
    "                  filepath='./training_checkpoints/ckpt_{epoch}',\n",
    "                  save_weights_only=True\n",
    "             ),\n",
    "            tf.keras.callbacks.LearningRateScheduler(\n",
    "                step_decay, \n",
    "                verbose=1\n",
    "            ),\n",
    "]\n",
    "\n",
    "# train the model\n",
    "history = model.fit(train_specs, train_labels, epochs=100, callbacks=callbacks,\n",
    "                   validation_data=(val_specs, val_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "90b0a369-76f8-49a1-8431-a2774b10e0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EVAULATION PORTION OF THE MODEL\n",
    "# run test data through model with ground truths\n",
    "# test_loss, test_acc = model.evaluate(test_specs,  test_labels, verbose=2)\n",
    "\n",
    "# get predictions of test spectrograms \n",
    "predictions = model.predict(test_specs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "ad8a5ea0-3462-491f-b1d0-2c74eea8411f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(predictions[7])\n",
    "# print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "7a68fbba-85fc-419c-8197-825c2a9a1a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to calculate accuracies of the model\n",
    "# outputs true positive, false positives, true negatives and false negatives\n",
    "# compares ground truths to predictions for specified instrument\n",
    "def counters(ground_truths, predictions, instrument):\n",
    "    \n",
    "    # initialise counters for each outcome\n",
    "    TP = 0\n",
    "    TN = 0\n",
    "    FP = 0\n",
    "    FN = 0\n",
    "    \n",
    "    # cycle through output predictions\n",
    "    for i in range(len(predictions)):\n",
    "        \n",
    "        # check if prediction matches specified instrument\n",
    "        if predictions[i] == instrument:\n",
    "            \n",
    "            # if ground truth matches instrument, add one to true positives\n",
    "            if ground_truths[i] == instrument:\n",
    "                TP += 1\n",
    "                \n",
    "            # else if ground truth does not match instrument, add one to false positives\n",
    "            if ground_truths[i] != instrument:\n",
    "                FP += 1\n",
    "                print(\"False Positive for: \", instrument)\n",
    "                print(ground_truths[i])\n",
    "            \n",
    "        # check if prediction does not match specified instrument\n",
    "        if predictions[i] != instrument:\n",
    "            \n",
    "            # if ground truth also does not match instrument, add one to true negatives\n",
    "            if ground_truths != instrument:\n",
    "                TN += 1\n",
    "            \n",
    "            # else if ground truth is instrument, add one to false negatives\n",
    "            if ground_truths[i] == instrument:\n",
    "                FN += 1\n",
    "                # print(\"False Negative for: \", instrument)\n",
    "                # print(predictions[i])\n",
    "            \n",
    "    return TP, FP, TN, FN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "c488378b-7fc8-42da-8f26-219cd64fafdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False Positive for:  Cello\n",
      "Acoustic Guitar\n",
      "False Positive for:  Cello\n",
      "Electric Guitar\n",
      "False Positive for:  Cello\n",
      "Voice\n",
      "False Positive for:  Cello\n",
      "Piano\n",
      "False Positive for:  Cello\n",
      "Voice\n",
      "Cello TP: 25\n",
      "Cello FP: 5\n",
      "Cello TN: 250\n",
      "Cello FN: 3\n",
      " \n",
      "False Positive for:  Clarinet\n",
      "Violin\n",
      "False Positive for:  Clarinet\n",
      "Flute\n",
      "False Positive for:  Clarinet\n",
      "Electric Guitar\n",
      "False Positive for:  Clarinet\n",
      "Flute\n",
      "False Positive for:  Clarinet\n",
      "Organ\n",
      "False Positive for:  Clarinet\n",
      "Organ\n",
      "False Positive for:  Clarinet\n",
      "Sax\n",
      "False Positive for:  Clarinet\n",
      "Flute\n",
      "False Positive for:  Clarinet\n",
      "Violin\n",
      "False Positive for:  Clarinet\n",
      "Violin\n",
      "Clarinet TP: 22\n",
      "Clarinet FP: 10\n",
      "Clarinet TN: 248\n",
      "Clarinet FN: 5\n",
      " \n",
      "False Positive for:  Flute\n",
      "Acoustic Guitar\n",
      "False Positive for:  Flute\n",
      "Acoustic Guitar\n",
      "Flute TP: 15\n",
      "Flute FP: 2\n",
      "Flute TN: 263\n",
      "Flute FN: 8\n",
      " \n",
      "False Positive for:  Acoustic Guitar\n",
      "Clarinet\n",
      "False Positive for:  Acoustic Guitar\n",
      "Sax\n",
      "False Positive for:  Acoustic Guitar\n",
      "Electric Guitar\n",
      "False Positive for:  Acoustic Guitar\n",
      "Flute\n",
      "Acoustic Guitar TP: 14\n",
      "Acoustic Guitar FP: 4\n",
      "Acoustic Guitar TN: 262\n",
      "Acoustic Guitar FN: 7\n",
      " \n",
      "False Positive for:  Electric Guitar\n",
      "Cello\n",
      "False Positive for:  Electric Guitar\n",
      "Flute\n",
      "False Positive for:  Electric Guitar\n",
      "Cello\n",
      "Electric Guitar TP: 15\n",
      "Electric Guitar FP: 3\n",
      "Electric Guitar TN: 262\n",
      "Electric Guitar FN: 7\n",
      " \n",
      "False Positive for:  Organ\n",
      "Clarinet\n",
      "False Positive for:  Organ\n",
      "Flute\n",
      "False Positive for:  Organ\n",
      "Flute\n",
      "Organ TP: 23\n",
      "Organ FP: 3\n",
      "Organ TN: 254\n",
      "Organ FN: 5\n",
      " \n",
      "False Positive for:  Piano\n",
      "Violin\n",
      "False Positive for:  Piano\n",
      "Sax\n",
      "False Positive for:  Piano\n",
      "Acoustic Guitar\n",
      "False Positive for:  Piano\n",
      "Cello\n",
      "False Positive for:  Piano\n",
      "Violin\n",
      "False Positive for:  Piano\n",
      "Electric Guitar\n",
      "False Positive for:  Piano\n",
      "Electric Guitar\n",
      "False Positive for:  Piano\n",
      "Violin\n",
      "False Positive for:  Piano\n",
      "Clarinet\n",
      "Piano TP: 20\n",
      "Piano FP: 9\n",
      "Piano TN: 251\n",
      "Piano FN: 2\n",
      " \n",
      "False Positive for:  Sax\n",
      "Acoustic Guitar\n",
      "False Positive for:  Sax\n",
      "Organ\n",
      "False Positive for:  Sax\n",
      "Electric Guitar\n",
      "False Positive for:  Sax\n",
      "Acoustic Guitar\n",
      "Sax TP: 24\n",
      "Sax FP: 4\n",
      "Sax TN: 252\n",
      "Sax FN: 4\n",
      " \n",
      "False Positive for:  Trumpet\n",
      "Organ\n",
      "False Positive for:  Trumpet\n",
      "Clarinet\n",
      "False Positive for:  Trumpet\n",
      "Clarinet\n",
      "False Positive for:  Trumpet\n",
      "Organ\n",
      "False Positive for:  Trumpet\n",
      "Voice\n",
      "Trumpet TP: 27\n",
      "Trumpet FP: 5\n",
      "Trumpet TN: 248\n",
      "Trumpet FN: 0\n",
      " \n",
      "False Positive for:  Violin\n",
      "Acoustic Guitar\n",
      "False Positive for:  Violin\n",
      "Electric Guitar\n",
      "False Positive for:  Violin\n",
      "Piano\n",
      "Violin TP: 10\n",
      "Violin FP: 3\n",
      "Violin TN: 267\n",
      "Violin FN: 6\n",
      " \n",
      "False Positive for:  Voice\n",
      "Flute\n",
      "False Positive for:  Voice\n",
      "Sax\n",
      "Voice TP: 11\n",
      "Voice FP: 2\n",
      "Voice TN: 267\n",
      "Voice FN: 3\n",
      " \n",
      "Drums TP: 24\n",
      "Drums FP: 0\n",
      "Drums TN: 256\n",
      "Drums FN: 0\n",
      " \n",
      "Other TP: 0\n",
      "Other FP: 0\n",
      "Other TN: 280\n",
      "Other FN: 0\n",
      " \n"
     ]
    }
   ],
   "source": [
    "# get number of predictions made\n",
    "no_of_predictions = len(predictions)\n",
    "\n",
    "# intialise arrays for storing labels and classifications as words\n",
    "true_labels = []\n",
    "classification = []\n",
    "\n",
    "# loop through all predictions\n",
    "for i in range(no_of_predictions):  \n",
    "    # convert vector labels to word labels\n",
    "    new_true = vector_to_label(test_labels[i])\n",
    "    # print(new_true)\n",
    "    true_labels.append(new_true)\n",
    "    \n",
    "    # get index where vector equals one\n",
    "    # vector should be 1-hot encoded\n",
    "    label_no = np.argmax(predictions[i])\n",
    "    # output word classification to array\n",
    "    # print(label_names[label_no])\n",
    "    classification.append(label_names[label_no])\n",
    "    \n",
    "# check stats for classification vs ground truth for each instrument\n",
    "for label in label_names:\n",
    "    TP, FP, TN, FN = counters(true_labels, classification, label)\n",
    "\n",
    "    print(label, \"TP:\", TP)\n",
    "    print(label, \"FP:\",  FP)\n",
    "    print(label, \"TN:\", TN)\n",
    "    print(label, \"FN:\", FN)\n",
    "    print(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "0a1397bc-9b87-41da-b910-95974ae17f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPEN-UNMIX PORTION OF THE CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "a9858cfe-e2ca-413d-8394-1a7edfb4cbd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO:\n",
    "\n",
    "# IMPORT CASE STUDY FILE AND SEND THROUGH OPEN-UNMIX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "528e7838-802c-4734-a8ba-f3f9b76a6027",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO:\n",
    "\n",
    "# add potion to upload input file\n",
    "\n",
    "# test_audio = \"bensound-clapandyell.mp3\"\n",
    "\n",
    "# case_study_snd, sr = librosa.load(test_audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "e0e89505-c325-4738-90a5-c48db81d3a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialise array for storing output audio\n",
    "# audio = []\n",
    "\n",
    "# # having some issues with that there sample rate\n",
    "# # sr = 22050\n",
    "\n",
    "# estimates = predict.separate(\n",
    "#     torch.as_tensor(case_study_snd).float(),\n",
    "#     rate = sr,\n",
    "#     device = device\n",
    "# )\n",
    "# for target, estimates in estimates.items():\n",
    "#     print(target)\n",
    "#     output = estimates.detach().cpu().numpy()[0]\n",
    "#     audio.append(output)\n",
    "#     display(Audio(output, rate=sr*2))\n",
    "#     # had to transpose output for it to not be empty\n",
    "#     # file_name = target + '.wav'\n",
    "#     # soundfile.write(file_name, audio.T, 16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b14f4e9a-95b1-4a4f-84a9-ab8580393c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO:\n",
    "\n",
    "# SDR etc evaluations on Open-Unmix output\n",
    "# museval toolkit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a5c0ad24-029a-40c3-b4c2-8e38b4a47046",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO:\n",
    "\n",
    "# PRE-PROCESS EACH OUTPUT TO BE CORRECT LENGTH\n",
    "# CONVERT SECTIONS TO SPECTROGRAMS\n",
    "# SEND ALL CUTS THROUGH CNN\n",
    "# TOTAL UP CLASSIFICATIONS FOR CUTS THAT ARE ALL SAME SOURCE\n",
    "# OUTPUT CLASSIFICATION AS MOST COMMON FOR THAT SOURCE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ca350a90-9368-412e-8d30-f8df5ca578cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio = []\n",
    "\n",
    "filenames = [\"vocals.wav\",\"drums.wav\",\"bass.wav\",\"other.wav\"]\n",
    "\n",
    "for file in filenames:    \n",
    "    snd, sr = librosa.load(file)\n",
    "    audio.append(snd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3ce78cd1-de04-4114-8b4f-7d0e398c23c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "print(len(audio))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f083ebb7-ce6a-4837-a152-97f19276b262",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 : 58\n",
      "1 : 58\n",
      "2 : 58\n",
      "3 : 58\n",
      "Total no of spectrograms:  232\n"
     ]
    }
   ],
   "source": [
    "three_seconds = 3 * sr\n",
    "\n",
    "case_study_spectrograms = []\n",
    "# compute mel spec parameters\n",
    "win_len = int(0.05 * sr)\n",
    "hop_len = int(0.025 * sr)\n",
    "n_mels = 64\n",
    "\n",
    "for n, output in enumerate(audio):\n",
    "    spec_counter = 0\n",
    "    \n",
    "    # output_mono = librosa.to_mono(output)\n",
    "    \n",
    "    for i in range(len(snd)):\n",
    "        if (i % three_seconds) == 0:\n",
    "\n",
    "            # compute mel spectrogram from raw audio\n",
    "            spec = mel_spec(output[i:i+three_seconds], sr=sr, n_mels=n_mels, hop_length=hop_len, \n",
    "                                                  win_length=win_len, window='hann')\n",
    "\n",
    "            if(spec.shape[1] < 121):\n",
    "                continue\n",
    "            \n",
    "            # rescale output range to be logarithmic\n",
    "            # corresponds to human perception\n",
    "            spec = tfio.audio.dbscale(spec, top_db=80)\n",
    "\n",
    "            # augment spectrogram in terms of frequency\n",
    "            augmented_spec = tfio.audio.freq_mask(spec, param=10)\n",
    "\n",
    "            # convert to original spectrogram to tensor\n",
    "            # add new axis to account for channels\n",
    "            spec = spec[..., tf.newaxis]\n",
    "            spec = tf.convert_to_tensor(spec)\n",
    "            # reshape to tensor shape\n",
    "            spec = tf.reshape(spec, (64, 121, 1))\n",
    "           \n",
    "            case_study_spectrograms.append(spec)\n",
    "            spec_counter += 1\n",
    "        \n",
    "    print(n, \":\", spec_counter)\n",
    "    \n",
    "print(\"Total no of spectrograms: \", len(case_study_spectrograms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a77ccec3-60d7-4c74-8605-b60da5f8110d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63.44747\n"
     ]
    }
   ],
   "source": [
    "# get largest value from across spectrograms\n",
    "case_study_max = np.amax(case_study_spectrograms)\n",
    "\n",
    "print(case_study_max)\n",
    "\n",
    "# normalise all specs according to maximum value\n",
    "normalised_case_study = np.abs(case_study_spectrograms // case_study_max)\n",
    "normalised_case_study = np.where(np.isnan(normalised_case_study), 0, normalised_case_study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "4a80f336-1ce0-4db1-845b-4717a1acfd28",
   "metadata": {},
   "outputs": [],
   "source": [
    "case_study_predictions = model.predict(normalised_case_study)\n",
    "\n",
    "case_study_classifications = []\n",
    "\n",
    "for k in range(len(case_study_predictions)):\n",
    "    case_label_no = np.argmax(predictions[k])\n",
    "    # output word classification to array\n",
    "    case_study_classifications.append(label_names[case_label_no])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "ad14dee3-ae93-4155-8587-931fb96236e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array(['Acoustic Guitar', 'Cello', 'Clarinet', 'Drums', 'Electric Guitar',\n",
      "       'Flute', 'Organ', 'Piano', 'Sax', 'Trumpet', 'Violin', 'Voice'],\n",
      "      dtype='<U15'), array([ 6,  2,  5, 12,  5,  2,  3,  3,  8,  6,  3,  3]))\n",
      "Drums\n",
      "(array(['Acoustic Guitar', 'Cello', 'Clarinet', 'Drums', 'Electric Guitar',\n",
      "       'Flute', 'Organ', 'Piano', 'Sax', 'Trumpet', 'Violin', 'Voice'],\n",
      "      dtype='<U15'), array([ 3,  2,  4, 11,  4,  3,  7,  5,  5,  3,  5,  6]))\n",
      "Drums\n",
      "(array(['Cello', 'Clarinet', 'Drums', 'Electric Guitar', 'Flute', 'Organ',\n",
      "       'Piano', 'Sax', 'Trumpet', 'Violin', 'Voice'], dtype='<U15'), array([4, 5, 8, 7, 3, 6, 3, 8, 9, 3, 2]))\n",
      "Trumpet\n",
      "(array(['Acoustic Guitar', 'Cello', 'Clarinet', 'Drums', 'Electric Guitar',\n",
      "       'Flute', 'Organ', 'Piano', 'Sax', 'Trumpet', 'Violin', 'Voice'],\n",
      "      dtype='<U15'), array([4, 8, 5, 7, 5, 6, 4, 5, 5, 2, 3, 4]))\n",
      "Cello\n"
     ]
    }
   ],
   "source": [
    "voice_class = case_study_classifications[:58]\n",
    "drums_class = case_study_classifications[58:116]\n",
    "bass_class = case_study_classifications[116:174]\n",
    "other_class = case_study_classifications[174:]\n",
    "\n",
    "unique_voice = np.unique(voice_class, return_counts=True)\n",
    "print(unique_voice)\n",
    "max_value_location = np.argmax(unique_voice[1])\n",
    "final_voice_classification = unique_voice[0][max_value_location]\n",
    "print(final_voice_classification)\n",
    "\n",
    "unique_drums = np.unique(drums_class, return_counts=True)\n",
    "print(unique_drums)\n",
    "max_value_location_d = np.argmax(unique_drums[1])\n",
    "final_drums_classification = unique_drums[0][max_value_location_d]\n",
    "print(final_drums_classification)\n",
    "\n",
    "unique_bass = np.unique(bass_class, return_counts=True)\n",
    "print(unique_bass)\n",
    "max_value_location_b = np.argmax(unique_bass[1])\n",
    "final_bass_classification = unique_bass[0][max_value_location_b]\n",
    "print(final_bass_classification)\n",
    "\n",
    "unique_other = np.unique(other_class, return_counts=True)\n",
    "print(unique_other)\n",
    "max_value_location_o = np.argmax(unique_other[1])\n",
    "final_other_classification = unique_other[0][max_value_location_o]\n",
    "print(final_other_classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab841c6-d21f-47c6-931b-1e49456c2a0f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
